{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iter 0: Loss: 104.3598\n",
      "Epoch 1, Iter 1000: Loss: 66.9683\n",
      "Epoch 1, Iter 2000: Loss: 60.1484\n",
      "Epoch 1, Iter 3000: Loss: 57.1521\n",
      "Epoch 1, Iter 4000: Loss: 55.0342\n",
      "Epoch 1, Iter 5000: Loss: 56.2506\n",
      "Epoch 1, Iter 6000: Loss: 78.6945\n",
      "Epoch 1, Iter 7000: Loss: 72.1564\n",
      "Epoch 1, Iter 8000: Loss: 82.6578\n",
      "Epoch 1, Iter 9000: Loss: 81.1758\n",
      "Epoch 1, Iter 10000: Loss: 78.2070\n",
      "Epoch 1, Iter 11000: Loss: 84.1926\n",
      "Epoch 1, Iter 12000: Loss: 81.0462\n",
      "Epoch 1, Iter 13000: Loss: 81.7593\n",
      "Epoch 1, Iter 14000: Loss: 79.7423\n",
      "Epoch 1, Iter 15000: Loss: 81.1464\n",
      "Epoch 1, Iter 16000: Loss: 80.4064\n",
      "Epoch 1, Iter 17000: Loss: 79.6246\n",
      "Epoch 1, Iter 18000: Loss: 78.3718\n",
      "Epoch 1, Iter 19000: Loss: 77.7721\n",
      "Epoch 1, Iter 20000: Loss: 77.7649\n",
      "Epoch 1, Iter 21000: Loss: 76.9936\n",
      "Epoch 1, Iter 22000: Loss: 82.8896\n",
      "Epoch 1, Iter 23000: Loss: 85.0461\n",
      "Epoch 1, Iter 24000: Loss: 81.6167\n",
      "Epoch 1, Iter 25000: Loss: 76.5984\n",
      "Epoch 1, Iter 26000: Loss: 84.4593\n",
      "Epoch 1, Iter 27000: Loss: 79.5354\n",
      "Epoch 1, Iter 28000: Loss: 79.5968\n",
      "Epoch 1, Iter 29000: Loss: 79.1755\n",
      "Epoch 1, Iter 30000: Loss: 83.8168\n",
      "Epoch 1, Iter 31000: Loss: 78.7505\n",
      "Epoch 1, Iter 32000: Loss: 80.1773\n",
      "Epoch 1, Iter 33000: Loss: 76.8574\n",
      "Epoch 1, Iter 34000: Loss: 80.2207\n",
      "Epoch 1, Iter 35000: Loss: 76.7915\n",
      "Epoch 1, Iter 36000: Loss: 78.7973\n",
      "Epoch 1, Iter 37000: Loss: 79.7013\n",
      "Epoch 1, Iter 38000: Loss: 77.6888\n",
      "Epoch 1, Iter 39000: Loss: 75.6063\n",
      "Epoch 1, Iter 40000: Loss: 76.6313\n",
      "Epoch 1, Iter 41000: Loss: 78.7173\n",
      "Epoch 1, Iter 42000: Loss: 77.0741\n",
      "Epoch 1, Iter 43000: Loss: 77.8514\n",
      "Epoch 1, Iter 44000: Loss: 77.5954\n",
      "Epoch 2, Iter 0: Loss: 77.0469\n",
      "Epoch 2, Iter 1000: Loss: 76.4471\n",
      "Epoch 2, Iter 2000: Loss: 74.3924\n",
      "Epoch 2, Iter 3000: Loss: 72.7271\n",
      "Epoch 2, Iter 4000: Loss: 73.9502\n",
      "Epoch 2, Iter 5000: Loss: 73.6669\n",
      "Epoch 2, Iter 6000: Loss: 74.7498\n",
      "Epoch 2, Iter 7000: Loss: 77.1814\n",
      "Epoch 2, Iter 8000: Loss: 79.4895\n",
      "Epoch 2, Iter 9000: Loss: 82.7593\n",
      "Epoch 2, Iter 10000: Loss: 79.8930\n",
      "Epoch 2, Iter 11000: Loss: 81.7359\n",
      "Epoch 2, Iter 12000: Loss: 80.0728\n",
      "Epoch 2, Iter 13000: Loss: 79.4176\n",
      "Epoch 2, Iter 14000: Loss: 78.3753\n",
      "Epoch 2, Iter 15000: Loss: 77.4504\n",
      "Epoch 2, Iter 16000: Loss: 76.4968\n",
      "Epoch 2, Iter 17000: Loss: 76.2216\n",
      "Epoch 2, Iter 18000: Loss: 76.3171\n",
      "Epoch 2, Iter 19000: Loss: 75.8436\n",
      "Epoch 2, Iter 20000: Loss: 75.2280\n",
      "Epoch 2, Iter 21000: Loss: 74.0943\n",
      "Epoch 2, Iter 22000: Loss: 73.7835\n",
      "Epoch 2, Iter 23000: Loss: 73.7284\n",
      "Epoch 2, Iter 24000: Loss: 73.7356\n",
      "Epoch 2, Iter 25000: Loss: 73.1337\n",
      "Epoch 2, Iter 26000: Loss: 77.9085\n",
      "Epoch 2, Iter 27000: Loss: 76.7316\n",
      "Epoch 2, Iter 28000: Loss: 76.2194\n",
      "Epoch 2, Iter 29000: Loss: 77.4634\n",
      "Epoch 2, Iter 30000: Loss: 77.0341\n",
      "Epoch 2, Iter 31000: Loss: 77.9928\n",
      "Epoch 2, Iter 32000: Loss: 77.6998\n",
      "Epoch 2, Iter 33000: Loss: 78.1899\n",
      "Epoch 2, Iter 34000: Loss: 78.4437\n",
      "Epoch 2, Iter 35000: Loss: 76.9210\n",
      "Epoch 2, Iter 36000: Loss: 75.6017\n",
      "Epoch 2, Iter 37000: Loss: 76.7609\n",
      "Epoch 2, Iter 38000: Loss: 76.5622\n",
      "Epoch 2, Iter 39000: Loss: 74.4189\n",
      "Epoch 2, Iter 40000: Loss: 72.3493\n",
      "Epoch 2, Iter 41000: Loss: 75.0243\n",
      "Epoch 2, Iter 42000: Loss: 73.8935\n",
      "Epoch 2, Iter 43000: Loss: 72.9358\n",
      "Epoch 2, Iter 44000: Loss: 75.8982\n",
      "Epoch 3, Iter 0: Loss: 75.6432\n",
      "Epoch 3, Iter 1000: Loss: 75.5410\n",
      "Epoch 3, Iter 2000: Loss: 73.5680\n",
      "Epoch 3, Iter 3000: Loss: 72.4742\n",
      "Epoch 3, Iter 4000: Loss: 74.0220\n",
      "Epoch 3, Iter 5000: Loss: 76.7120\n",
      "Epoch 3, Iter 6000: Loss: 79.8246\n",
      "Epoch 3, Iter 7000: Loss: 82.2055\n",
      "Epoch 3, Iter 8000: Loss: 74.7532\n",
      "Epoch 3, Iter 9000: Loss: 74.9442\n",
      "Epoch 3, Iter 10000: Loss: 73.8456\n",
      "Epoch 3, Iter 11000: Loss: 77.2437\n",
      "Epoch 3, Iter 12000: Loss: 74.9167\n",
      "Epoch 3, Iter 13000: Loss: 73.9168\n",
      "Epoch 3, Iter 14000: Loss: 72.4418\n",
      "Epoch 3, Iter 15000: Loss: 73.4814\n",
      "Epoch 3, Iter 16000: Loss: 73.2032\n",
      "Epoch 3, Iter 17000: Loss: 71.7933\n",
      "Epoch 3, Iter 18000: Loss: 72.4013\n",
      "Epoch 3, Iter 19000: Loss: 71.1728\n",
      "Epoch 3, Iter 20000: Loss: 70.9234\n",
      "Epoch 3, Iter 21000: Loss: 70.7668\n",
      "Epoch 3, Iter 22000: Loss: 71.9247\n",
      "Epoch 3, Iter 23000: Loss: 71.0702\n",
      "Epoch 3, Iter 24000: Loss: 69.4109\n",
      "Epoch 3, Iter 25000: Loss: 68.4838\n",
      "Epoch 3, Iter 26000: Loss: 73.2592\n",
      "Epoch 3, Iter 27000: Loss: 70.6916\n",
      "Epoch 3, Iter 28000: Loss: 70.8485\n",
      "Epoch 3, Iter 29000: Loss: 72.9873\n",
      "Epoch 3, Iter 30000: Loss: 69.3427\n",
      "Epoch 3, Iter 31000: Loss: 70.8168\n",
      "Epoch 3, Iter 32000: Loss: 69.1905\n",
      "Epoch 3, Iter 33000: Loss: 72.2943\n",
      "Epoch 3, Iter 34000: Loss: 70.8074\n",
      "Epoch 3, Iter 35000: Loss: 70.3338\n",
      "Epoch 3, Iter 36000: Loss: 70.0485\n",
      "Epoch 3, Iter 37000: Loss: 75.5341\n",
      "Epoch 3, Iter 38000: Loss: 72.3046\n",
      "Epoch 3, Iter 39000: Loss: 73.2960\n",
      "Epoch 3, Iter 40000: Loss: 69.6819\n",
      "Epoch 3, Iter 41000: Loss: 70.8007\n",
      "Epoch 3, Iter 42000: Loss: 69.0554\n",
      "Epoch 3, Iter 43000: Loss: 68.1393\n",
      "Epoch 3, Iter 44000: Loss: 70.9964\n",
      "Epoch 4, Iter 0: Loss: 70.7530\n",
      "Epoch 4, Iter 1000: Loss: 70.3487\n",
      "Epoch 4, Iter 2000: Loss: 69.7435\n",
      "Epoch 4, Iter 3000: Loss: 68.4521\n",
      "Epoch 4, Iter 4000: Loss: 68.7082\n",
      "Epoch 4, Iter 5000: Loss: 69.0426\n",
      "Epoch 4, Iter 6000: Loss: 69.9392\n",
      "Epoch 4, Iter 7000: Loss: 72.0945\n",
      "Epoch 4, Iter 8000: Loss: 69.5349\n",
      "Epoch 4, Iter 9000: Loss: 70.0913\n",
      "Epoch 4, Iter 10000: Loss: 69.2146\n",
      "Epoch 4, Iter 11000: Loss: 70.9730\n",
      "Epoch 4, Iter 12000: Loss: 73.2895\n",
      "Epoch 4, Iter 13000: Loss: 70.8861\n",
      "Epoch 4, Iter 14000: Loss: 69.5414\n",
      "Epoch 4, Iter 15000: Loss: 69.6354\n",
      "Epoch 4, Iter 16000: Loss: 69.4662\n",
      "Epoch 4, Iter 17000: Loss: 69.1631\n",
      "Epoch 4, Iter 18000: Loss: 70.2625\n",
      "Epoch 4, Iter 19000: Loss: 69.6880\n",
      "Epoch 4, Iter 20000: Loss: 69.1629\n",
      "Epoch 4, Iter 21000: Loss: 68.5546\n",
      "Epoch 4, Iter 22000: Loss: 68.7461\n",
      "Epoch 4, Iter 23000: Loss: 68.2458\n",
      "Epoch 4, Iter 24000: Loss: 66.7884\n",
      "Epoch 4, Iter 25000: Loss: 66.0798\n",
      "Epoch 4, Iter 26000: Loss: 69.1682\n",
      "Epoch 4, Iter 27000: Loss: 68.0158\n",
      "Epoch 4, Iter 28000: Loss: 68.7389\n",
      "Epoch 4, Iter 29000: Loss: 70.4760\n",
      "Epoch 4, Iter 30000: Loss: 67.5024\n",
      "Epoch 4, Iter 31000: Loss: 68.3706\n",
      "Epoch 4, Iter 32000: Loss: 66.9454\n",
      "Epoch 4, Iter 33000: Loss: 68.6131\n",
      "Epoch 4, Iter 34000: Loss: 68.3452\n",
      "Epoch 4, Iter 35000: Loss: 66.0331\n",
      "Epoch 4, Iter 36000: Loss: 66.1785\n",
      "Epoch 4, Iter 37000: Loss: 69.2551\n",
      "Epoch 4, Iter 38000: Loss: 67.8958\n",
      "Epoch 4, Iter 39000: Loss: 69.0730\n",
      "Epoch 4, Iter 40000: Loss: 64.7709\n",
      "Epoch 4, Iter 41000: Loss: 67.9777\n",
      "Epoch 4, Iter 42000: Loss: 65.2010\n",
      "Epoch 4, Iter 43000: Loss: 63.2663\n",
      "Epoch 4, Iter 44000: Loss: 66.5259\n",
      "Epoch 5, Iter 0: Loss: 66.7217\n",
      "Epoch 5, Iter 1000: Loss: 67.3146\n",
      "Epoch 5, Iter 2000: Loss: 66.4152\n",
      "Epoch 5, Iter 3000: Loss: 65.2789\n",
      "Epoch 5, Iter 4000: Loss: 64.1242\n",
      "Epoch 5, Iter 5000: Loss: 66.4041\n",
      "Epoch 5, Iter 6000: Loss: 67.2038\n",
      "Epoch 5, Iter 7000: Loss: 69.0613\n",
      "Epoch 5, Iter 8000: Loss: 65.2870\n",
      "Epoch 5, Iter 9000: Loss: 67.1601\n",
      "Epoch 5, Iter 10000: Loss: 65.9743\n",
      "Epoch 5, Iter 11000: Loss: 68.8983\n",
      "Epoch 5, Iter 12000: Loss: 68.5773\n",
      "Epoch 5, Iter 13000: Loss: 67.5398\n",
      "Epoch 5, Iter 14000: Loss: 67.1147\n",
      "Epoch 5, Iter 15000: Loss: 66.0255\n",
      "Epoch 5, Iter 16000: Loss: 66.5556\n",
      "Epoch 5, Iter 17000: Loss: 64.6432\n",
      "Epoch 5, Iter 18000: Loss: 68.3361\n",
      "Epoch 5, Iter 19000: Loss: 67.0668\n",
      "Epoch 5, Iter 20000: Loss: 65.3008\n",
      "Epoch 5, Iter 21000: Loss: 65.6117\n",
      "Epoch 5, Iter 22000: Loss: 66.5466\n",
      "Epoch 5, Iter 23000: Loss: 65.9684\n",
      "Epoch 5, Iter 24000: Loss: 65.2719\n",
      "Epoch 5, Iter 25000: Loss: 65.0098\n",
      "Epoch 5, Iter 26000: Loss: 67.1863\n",
      "Epoch 5, Iter 27000: Loss: 66.0939\n",
      "Epoch 5, Iter 28000: Loss: 67.1188\n",
      "Epoch 5, Iter 29000: Loss: 68.6471\n",
      "Epoch 5, Iter 30000: Loss: 64.6307\n",
      "Epoch 5, Iter 31000: Loss: 66.4718\n",
      "Epoch 5, Iter 32000: Loss: 64.8265\n",
      "Epoch 5, Iter 33000: Loss: 66.7947\n",
      "Epoch 5, Iter 34000: Loss: 66.8498\n",
      "Epoch 5, Iter 35000: Loss: 62.9761\n",
      "Epoch 5, Iter 36000: Loss: 63.9873\n",
      "Epoch 5, Iter 37000: Loss: 67.4778\n",
      "Epoch 5, Iter 38000: Loss: 64.8289\n",
      "Epoch 5, Iter 39000: Loss: 67.4306\n",
      "Epoch 5, Iter 40000: Loss: 63.0727\n",
      "Epoch 5, Iter 41000: Loss: 65.9404\n",
      "Epoch 5, Iter 42000: Loss: 64.0836\n",
      "Epoch 5, Iter 43000: Loss: 61.9495\n",
      "Epoch 5, Iter 44000: Loss: 65.2064\n",
      "Epoch 6, Iter 0: Loss: 64.3100\n",
      "Epoch 6, Iter 1000: Loss: 64.8663\n",
      "Epoch 6, Iter 2000: Loss: 64.5858\n",
      "Epoch 6, Iter 3000: Loss: 64.0559\n",
      "Epoch 6, Iter 4000: Loss: 62.3721\n",
      "Epoch 6, Iter 5000: Loss: 64.1329\n",
      "Epoch 6, Iter 6000: Loss: 65.2700\n",
      "Epoch 6, Iter 7000: Loss: 67.3668\n",
      "Epoch 6, Iter 8000: Loss: 63.8627\n",
      "Epoch 6, Iter 9000: Loss: 65.4495\n",
      "Epoch 6, Iter 10000: Loss: 63.7248\n",
      "Epoch 6, Iter 11000: Loss: 66.4575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Iter 12000: Loss: 65.9749\n",
      "Epoch 6, Iter 13000: Loss: 66.7504\n",
      "Epoch 6, Iter 14000: Loss: 67.1111\n",
      "Epoch 6, Iter 15000: Loss: 65.6233\n",
      "Epoch 6, Iter 16000: Loss: 68.4883\n",
      "Epoch 6, Iter 17000: Loss: 65.9220\n",
      "Epoch 6, Iter 18000: Loss: 66.9174\n",
      "Epoch 6, Iter 19000: Loss: 66.4423\n",
      "Epoch 6, Iter 20000: Loss: 64.8670\n",
      "Epoch 6, Iter 21000: Loss: 65.1962\n",
      "Epoch 6, Iter 22000: Loss: 65.5347\n",
      "Epoch 6, Iter 23000: Loss: 65.4976\n",
      "Epoch 6, Iter 24000: Loss: 64.4783\n",
      "Epoch 6, Iter 25000: Loss: 63.7826\n",
      "Epoch 6, Iter 26000: Loss: 68.1549\n",
      "Epoch 6, Iter 27000: Loss: 68.9528\n",
      "Epoch 6, Iter 28000: Loss: 67.3541\n",
      "Epoch 6, Iter 29000: Loss: 68.2187\n",
      "Epoch 6, Iter 30000: Loss: 64.9791\n",
      "Epoch 6, Iter 31000: Loss: 66.7331\n",
      "Epoch 6, Iter 32000: Loss: 64.5831\n",
      "Epoch 6, Iter 33000: Loss: 65.8320\n",
      "Epoch 6, Iter 34000: Loss: 65.7681\n",
      "Epoch 6, Iter 35000: Loss: 64.5784\n",
      "Epoch 6, Iter 36000: Loss: 63.2051\n",
      "Epoch 6, Iter 37000: Loss: 66.5704\n",
      "Epoch 6, Iter 38000: Loss: 65.4894\n",
      "Epoch 6, Iter 39000: Loss: 66.8727\n",
      "Epoch 6, Iter 40000: Loss: 61.7828\n",
      "Epoch 6, Iter 41000: Loss: 65.5476\n",
      "Epoch 6, Iter 42000: Loss: 62.7717\n",
      "Epoch 6, Iter 43000: Loss: 60.5602\n",
      "Epoch 6, Iter 44000: Loss: 64.4791\n",
      "Epoch 7, Iter 0: Loss: 66.9558\n",
      "Epoch 7, Iter 1000: Loss: 65.7297\n",
      "Epoch 7, Iter 2000: Loss: 65.2997\n",
      "Epoch 7, Iter 3000: Loss: 64.0027\n",
      "Epoch 7, Iter 4000: Loss: 62.6437\n",
      "Epoch 7, Iter 5000: Loss: 64.3911\n",
      "Epoch 7, Iter 6000: Loss: 65.8784\n",
      "Epoch 7, Iter 7000: Loss: 66.4468\n",
      "Epoch 7, Iter 8000: Loss: 63.3669\n",
      "Epoch 7, Iter 9000: Loss: 66.4147\n",
      "Epoch 7, Iter 10000: Loss: 64.9526\n",
      "Epoch 7, Iter 11000: Loss: 67.2672\n",
      "Epoch 7, Iter 12000: Loss: 67.2814\n",
      "Epoch 7, Iter 13000: Loss: 66.2695\n",
      "Epoch 7, Iter 14000: Loss: 65.7202\n",
      "Epoch 7, Iter 15000: Loss: 64.5354\n",
      "Epoch 7, Iter 16000: Loss: 65.6627\n",
      "Epoch 7, Iter 17000: Loss: 64.6690\n",
      "Epoch 7, Iter 18000: Loss: 65.7593\n",
      "Epoch 7, Iter 19000: Loss: 65.1241\n",
      "Epoch 7, Iter 20000: Loss: 63.8119\n",
      "Epoch 7, Iter 21000: Loss: 64.8132\n",
      "Epoch 7, Iter 22000: Loss: 64.5655\n",
      "Epoch 7, Iter 23000: Loss: 65.0332\n",
      "Epoch 7, Iter 24000: Loss: 63.8571\n",
      "Epoch 7, Iter 25000: Loss: 62.9251\n",
      "Epoch 7, Iter 26000: Loss: 65.8299\n",
      "Epoch 7, Iter 27000: Loss: 65.1094\n",
      "Epoch 7, Iter 28000: Loss: 65.5582\n",
      "Epoch 7, Iter 29000: Loss: 67.1439\n",
      "Epoch 7, Iter 30000: Loss: 64.0495\n",
      "Epoch 7, Iter 31000: Loss: 65.3874\n",
      "Epoch 7, Iter 32000: Loss: 63.9620\n",
      "Epoch 7, Iter 33000: Loss: 65.7082\n",
      "Epoch 7, Iter 34000: Loss: 65.0991\n",
      "Epoch 7, Iter 35000: Loss: 62.4044\n",
      "Epoch 7, Iter 36000: Loss: 62.4450\n",
      "Epoch 7, Iter 37000: Loss: 65.6115\n",
      "Epoch 7, Iter 38000: Loss: 63.3742\n",
      "Epoch 7, Iter 39000: Loss: 66.3781\n",
      "Epoch 7, Iter 40000: Loss: 61.6401\n",
      "Epoch 7, Iter 41000: Loss: 64.7330\n",
      "Epoch 7, Iter 42000: Loss: 62.3705\n",
      "Epoch 7, Iter 43000: Loss: 59.9788\n",
      "Epoch 7, Iter 44000: Loss: 64.7783\n",
      "Epoch 8, Iter 0: Loss: 63.5577\n",
      "Epoch 8, Iter 1000: Loss: 65.0482\n",
      "Epoch 8, Iter 2000: Loss: 63.7083\n",
      "Epoch 8, Iter 3000: Loss: 62.9934\n",
      "Epoch 8, Iter 4000: Loss: 61.7393\n",
      "Epoch 8, Iter 5000: Loss: 63.9216\n",
      "Epoch 8, Iter 6000: Loss: 63.9218\n",
      "Epoch 8, Iter 7000: Loss: 65.6391\n",
      "Epoch 8, Iter 8000: Loss: 63.0134\n",
      "Epoch 8, Iter 9000: Loss: 64.5701\n",
      "Epoch 8, Iter 10000: Loss: 63.9758\n",
      "Epoch 8, Iter 11000: Loss: 67.0828\n",
      "Epoch 8, Iter 12000: Loss: 65.9708\n",
      "Epoch 8, Iter 13000: Loss: 66.0168\n",
      "Epoch 8, Iter 14000: Loss: 65.6772\n",
      "Epoch 8, Iter 15000: Loss: 64.5740\n",
      "Epoch 8, Iter 16000: Loss: 64.4100\n",
      "Epoch 8, Iter 17000: Loss: 66.0320\n",
      "Epoch 8, Iter 18000: Loss: 67.2443\n",
      "Epoch 8, Iter 19000: Loss: 65.8557\n",
      "Epoch 8, Iter 20000: Loss: 64.9722\n",
      "Epoch 8, Iter 21000: Loss: 65.0914\n",
      "Epoch 8, Iter 22000: Loss: 64.8597\n",
      "Epoch 8, Iter 23000: Loss: 64.7117\n",
      "Epoch 8, Iter 24000: Loss: 64.1186\n",
      "Epoch 8, Iter 25000: Loss: 63.4173\n",
      "Epoch 8, Iter 26000: Loss: 66.3992\n",
      "Epoch 8, Iter 27000: Loss: 65.2076\n",
      "Epoch 8, Iter 28000: Loss: 66.5918\n",
      "Epoch 8, Iter 29000: Loss: 67.5027\n",
      "Epoch 8, Iter 30000: Loss: 65.5397\n",
      "Epoch 8, Iter 31000: Loss: 66.2645\n",
      "Epoch 8, Iter 32000: Loss: 64.7253\n",
      "Epoch 8, Iter 33000: Loss: 65.3695\n",
      "Epoch 8, Iter 34000: Loss: 65.2762\n",
      "Epoch 8, Iter 35000: Loss: 61.6815\n",
      "Epoch 8, Iter 36000: Loss: 63.4874\n",
      "Epoch 8, Iter 37000: Loss: 66.8724\n",
      "Epoch 8, Iter 38000: Loss: 65.8655\n",
      "Epoch 8, Iter 39000: Loss: 67.0092\n",
      "Epoch 8, Iter 40000: Loss: 64.0530\n",
      "Epoch 8, Iter 41000: Loss: 68.1148\n",
      "Epoch 8, Iter 42000: Loss: 64.6528\n",
      "Epoch 8, Iter 43000: Loss: 63.4657\n",
      "Epoch 8, Iter 44000: Loss: 66.1622\n",
      "Epoch 9, Iter 0: Loss: 65.8075\n",
      "Epoch 9, Iter 1000: Loss: 66.5093\n",
      "Epoch 9, Iter 2000: Loss: 65.9840\n",
      "Epoch 9, Iter 3000: Loss: 64.2674\n",
      "Epoch 9, Iter 4000: Loss: 63.9936\n",
      "Epoch 9, Iter 5000: Loss: 65.4258\n",
      "Epoch 9, Iter 6000: Loss: 65.8183\n",
      "Epoch 9, Iter 7000: Loss: 69.1836\n",
      "Epoch 9, Iter 8000: Loss: 66.2122\n",
      "Epoch 9, Iter 9000: Loss: 67.7540\n",
      "Epoch 9, Iter 10000: Loss: 65.8921\n",
      "Epoch 9, Iter 11000: Loss: 71.8662\n",
      "Epoch 9, Iter 12000: Loss: 70.0215\n",
      "Epoch 9, Iter 13000: Loss: 69.5007\n",
      "Epoch 9, Iter 14000: Loss: 68.9262\n",
      "Epoch 9, Iter 15000: Loss: 66.4348\n",
      "Epoch 9, Iter 16000: Loss: 68.6777\n",
      "Epoch 9, Iter 17000: Loss: 67.7235\n",
      "Epoch 9, Iter 18000: Loss: 68.3931\n",
      "Epoch 9, Iter 19000: Loss: 68.6002\n",
      "Epoch 9, Iter 20000: Loss: 67.3259\n",
      "Epoch 9, Iter 21000: Loss: 65.9693\n",
      "Epoch 9, Iter 22000: Loss: 67.7120\n",
      "Epoch 9, Iter 23000: Loss: 66.8635\n",
      "Epoch 9, Iter 24000: Loss: 66.8924\n",
      "Epoch 9, Iter 25000: Loss: 64.7637\n",
      "Epoch 9, Iter 26000: Loss: 71.0605\n",
      "Epoch 9, Iter 27000: Loss: 68.2045\n",
      "Epoch 9, Iter 28000: Loss: 68.3664\n",
      "Epoch 9, Iter 29000: Loss: 69.2065\n",
      "Epoch 9, Iter 30000: Loss: 66.8623\n",
      "Epoch 9, Iter 31000: Loss: 67.9480\n",
      "Epoch 9, Iter 32000: Loss: 67.4243\n",
      "Epoch 9, Iter 33000: Loss: 67.4371\n",
      "Epoch 9, Iter 34000: Loss: 67.6063\n",
      "Epoch 9, Iter 35000: Loss: 65.7108\n",
      "Epoch 9, Iter 36000: Loss: 64.8045\n",
      "Epoch 9, Iter 37000: Loss: 68.4319\n",
      "Epoch 9, Iter 38000: Loss: 66.6419\n",
      "Epoch 9, Iter 39000: Loss: 68.3425\n",
      "Epoch 9, Iter 40000: Loss: 65.2004\n",
      "Epoch 9, Iter 41000: Loss: 67.0083\n",
      "Epoch 9, Iter 42000: Loss: 64.6968\n",
      "Epoch 9, Iter 43000: Loss: 63.0253\n",
      "Epoch 9, Iter 44000: Loss: 67.9009\n",
      "Epoch 10, Iter 0: Loss: 66.5546\n",
      "Epoch 10, Iter 1000: Loss: 67.0510\n",
      "Epoch 10, Iter 2000: Loss: 66.0203\n",
      "Epoch 10, Iter 3000: Loss: 65.1852\n",
      "Epoch 10, Iter 4000: Loss: 64.5932\n",
      "Epoch 10, Iter 5000: Loss: 65.9283\n",
      "Epoch 10, Iter 6000: Loss: 67.4380\n",
      "Epoch 10, Iter 7000: Loss: 67.8878\n",
      "Epoch 10, Iter 8000: Loss: 64.5578\n",
      "Epoch 10, Iter 9000: Loss: 67.0246\n",
      "Epoch 10, Iter 10000: Loss: 65.7002\n",
      "Epoch 10, Iter 11000: Loss: 68.9613\n",
      "Epoch 10, Iter 12000: Loss: 73.7722\n",
      "Epoch 10, Iter 13000: Loss: 72.4706\n",
      "Epoch 10, Iter 14000: Loss: 70.4312\n",
      "Epoch 10, Iter 15000: Loss: 69.5911\n",
      "Epoch 10, Iter 16000: Loss: 69.3204\n",
      "Epoch 10, Iter 17000: Loss: 68.5776\n",
      "Epoch 10, Iter 18000: Loss: 72.4624\n",
      "Epoch 10, Iter 19000: Loss: 71.1558\n",
      "Epoch 10, Iter 20000: Loss: 68.2671\n",
      "Epoch 10, Iter 21000: Loss: 67.6226\n",
      "Epoch 10, Iter 22000: Loss: 68.4406\n",
      "Epoch 10, Iter 23000: Loss: 68.5136\n",
      "Epoch 10, Iter 24000: Loss: 69.6103\n",
      "Epoch 10, Iter 25000: Loss: 68.8894\n",
      "Epoch 10, Iter 26000: Loss: 71.3205\n",
      "Epoch 10, Iter 27000: Loss: 70.2615\n",
      "Epoch 10, Iter 28000: Loss: 73.0974\n",
      "Epoch 10, Iter 29000: Loss: 72.5210\n",
      "Epoch 10, Iter 30000: Loss: 67.4312\n",
      "Epoch 10, Iter 31000: Loss: 68.2278\n",
      "Epoch 10, Iter 32000: Loss: 68.4773\n",
      "Epoch 10, Iter 33000: Loss: 69.7208\n",
      "Epoch 10, Iter 34000: Loss: 67.7906\n",
      "Epoch 10, Iter 35000: Loss: 64.8997\n",
      "Epoch 10, Iter 36000: Loss: 66.8023\n",
      "Epoch 10, Iter 37000: Loss: 68.7665\n",
      "Epoch 10, Iter 38000: Loss: 71.4682\n",
      "Epoch 10, Iter 39000: Loss: 69.6741\n",
      "Epoch 10, Iter 40000: Loss: 66.7104\n",
      "Epoch 10, Iter 41000: Loss: 68.5887\n",
      "Epoch 10, Iter 42000: Loss: 66.3000\n",
      "Epoch 10, Iter 43000: Loss: 64.6067\n",
      "Epoch 10, Iter 44000: Loss: 67.0337\n",
      "e, Vyteld been.\n",
      "Wy Write: was;\n",
      "\n",
      "SEHAN:ONCIO:\n",
      "Nivelpae,\n",
      "Whet Ang.\n",
      "\n",
      "paacee:\n",
      "The brid\n",
      "\n",
      "a aig\n",
      "shelyeer m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, corpus, hidden_size=128, seq_len=25, lr=1e-2, epochs=100):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            corpus {str} -- Entire text corpus\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            hidden_size {number} -- size of hidden state (default: {128})\n",
    "            seq_len {number} -- time steps to unroll for (default: {25})\n",
    "            lr {number} -- learning rate (default: {1e-3})\n",
    "            epochs {number} -- number of epochs to train for (default: {100})\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # create mapping from characters to numbers and back\n",
    "        chars = list(set(corpus))\n",
    "        self.data_size, self.input_size, self.output_size = len(corpus), len(chars), len(chars)\n",
    "        self.char_to_num = {c:i for i,c in enumerate(chars)}\n",
    "        self.num_to_char = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "        self.h = np.zeros((self.hidden_size , 1))\n",
    "\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size) * 0.01\n",
    "        self.W_hh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((self.hidden_size, 1))\n",
    "        self.b_y = np.zeros((self.output_size, 1))\n",
    "\n",
    "    def __loss(self, X, Y):\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(self.h)\n",
    "\n",
    "        # forward pass\n",
    "        loss = 0\n",
    "        for t in range(len(X)):\n",
    "            xs[t] = np.zeros((self.input_size, 1))\n",
    "            xs[t][X[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.W_xh, xs[t]) + np.dot(self.W_hh, hs[t-1]) + self.b_h)\n",
    "            ys[t] = np.dot(self.W_hy, hs[t]) + self.b_y\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "            loss += -np.log(ps[t][Y[t], 0])\n",
    "\n",
    "        # backward pass\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        delta = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(X))):\n",
    "            dy = np.copy(ps[t])\n",
    "            # backprop into y\n",
    "            dy[Y[t]] -= 1\n",
    "            dW_hy += np.dot(dy, hs[t].T)\n",
    "            db_y += dy\n",
    "\n",
    "            # backprop into h\n",
    "            dh = np.dot(self.W_hy.T, dy) + delta\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh\n",
    "            db_h += dh_raw\n",
    "            dW_hh += np.dot(dh_raw, hs[t-1].T)\n",
    "            dW_xh += np.dot(dh_raw, xs[t].T)\n",
    "\n",
    "            # update delta\n",
    "            delta = np.dot(self.W_hh.T, dh_raw)\n",
    "        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            # gradient clipping to prevent exploding gradient\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        # update last hidden state for sampling\n",
    "        self.h = hs[len(X) - 1]\n",
    "        return loss, dW_xh, dW_hh, dW_hy, db_h, db_y\n",
    "\n",
    "    def fit(self):\n",
    "        smoothed_loss = -np.log(1. / self.input_size) * self.seq_len\n",
    "        for e in range(self.epochs):\n",
    "            for p in range(np.floor(self.data_size / self.seq_len).astype(np.int64)):\n",
    "                # get a slice of data with length at most seq_len\n",
    "                x = [self.char_to_num[c] for c in self.corpus[p * self.seq_len:(p + 1) * self.seq_len]]\n",
    "                y = [self.char_to_num[c] for c in self.corpus[p * self.seq_len + 1:(p + 1) * self.seq_len + 1]]\n",
    "\n",
    "                # compute loss and gradients\n",
    "                loss, dW_xh, dW_hh, dW_hy, db_h, db_y = self.__loss(x, y)\n",
    "                smoothed_loss = smoothed_loss * 0.99 + loss * 0.01\n",
    "                if p % 1000 == 0: print('Epoch {0}, Iter {1}: Loss: {2:.4f}'.format(e+1, p, smoothed_loss))\n",
    "\n",
    "                # SGD update\n",
    "                for param, dparam in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y], [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n",
    "                    param += -self.lr * dparam\n",
    "\n",
    "    def sample(self, seed, n):\n",
    "        \"\"\"Generate text from the RNN\n",
    "        \n",
    "        Arguments:\n",
    "            seed {str} -- character to start the sequence with\n",
    "            n {int} -- length of sequence\n",
    "        \"\"\"\n",
    "        seq = []\n",
    "        h = self.h\n",
    "\n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x[self.char_to_num[seed]] = 1\n",
    "\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            y = np.dot(self.W_hy, h) + self.b_y\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "            # sample from the distribution\n",
    "            seq_t = np.random.choice(range(self.input_size), p=p.ravel())\n",
    "\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x[seq_t] = 1\n",
    "            seq.append(seq_t)\n",
    "        return ''.join(self.num_to_char[num] for num in seq)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('input.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    char_rnn = CharRNN(data, epochs=10)\n",
    "    char_rnn.fit()\n",
    "    print(char_rnn.sample(data[0], 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
