{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png\" width=\"400px\" align=\"center\"></a>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">Project: Character Modeling</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<font size=\"3\"><strong>In this notebook you will use TensorFlow to create a Recurrent Neural Network, to predict the next character in a string. You need to train your network using a CPU and using a GPU and benchmark the result to see which which device You have to write your code in empty cells in this notebook to make it complete, and then submit the notebook for peer-review.</strong></font>\n",
    "\n",
    "<h2>Table of Contents</h2>\n",
    "<ul>\n",
    "    <li><a href=\"#intro\">Introduction</a></li>\n",
    "    <li><a href=\"#lstm\">Long Short-Term Memory Model (LSTM) Architectures</a></li>\n",
    "    <li><a href=\"#cpu_vs_gpu\">Train your model using CPU and GPU</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#question_1\">Question 1: Complete the code to run it on CPU</a></li>\n",
    "            <li><a href=\"#question_2\">Question 2: Complete the code to run it on GPU</a></li>\n",
    "            <li><a href=\"#question_3\">Question 3: Compare the results</a></li>\n",
    "        </ol>    \n",
    "    </li>\n",
    "</ul>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>This code is supposed to implement a Recurrent Neural Network with LSTM units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.</p>  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "<p>This code is based on this <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">blog</a>, and the code is an step-by-step implementation of the <a href=\"https://github.com/crazydonkey200/tensorflow-char-rnn\">character-level implimentation</a>.</p>\n",
    "\n",
    "<p>I recommend you to complete the \"<a href=\"https://www.edx.org/course/deep-learning-with-tensorflow\">Deep Learning with TensorFlow</a>\" course for this project.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, lets import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Libraries\n",
    "<ul>\n",
    "    <li><b>os</b>: is an module that allows us to interact with the operating systerm, in particular we will use it to set the path in which we will be storing our input file, tensor file and vocab file</li>\n",
    "    <li><b>time</b>: is a library that allows us to access the clock time of our machine, we will use it to measure the performance of training our model with a CPU, versus training our model with a GPU</li>\n",
    "    <li><b>cPickle</b>: is a library for serializing and deserializing python objects, we will use the <b>dump()</b> method in cPickle to serialize our objects when saving them, and <b>load()</b> method in cPickle to deserialize our objects when loading.</li>\n",
    "    <li><b>codec</b>: is a library that deals with character encoding, we will use the <b>open()</b> method as it is recommended when opening encoded text files.</li>\n",
    "    <li><b>collections</b>: is a library that implements high performance container types, we will use the <b>Counter</b> object to get a collection of frequencies for our characters</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Downloading the input data</h3>\n",
    "Lets download the input file, and take a look at some parts of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-18 19:45:09 URL:https://public.boxcloud.com/d/1/b1!S7lrbiu-nNj8ca_2HPtnY-0sgSQ6sc7-Ag1LJaMlz2l76EZuhBV0jfUbTUXilz2rk-ViSXWYGJ-WXqGw1UX6UNjNlZrvqRZprxRlRIIu-RgJh_uUdhd3iiy80AL3mpEXrmWYnzfKRlZlK4hd37S5wnwz29l6dXMHPmQbBj4nqhgI-0G2YXEkS8K5PLfitaWf8vOk9nsDO9ihjZakOPP0YOYdyF5JEj3XuqHlvemLMoce6Izj0IRW8bLsAoMSHLw3IROmpE7lw5cHZwZtksXZcjw8idfeMEQ_ae9jF1EBOA6Nh9gY4kxp0ljTb3eKvAHpX9BMcYyvsKJdg2U6A4vM8xFfJgA0qhp6cGRYiv1mRuQrroPqOtoTqrg1AKAdGbm_AXK-Zu87_JvzwuWPlIKMsa-cEjutrPVUcVqXzCBWUAVk3Y_w_d7-uwQDmw3dwyJ4QlR0Ff1TYP3S17lA2sMqi4eWwUyvAykUSWKNfotc0E9rwnMWGvKQbbfBXsDI7dsdZVFnGBevGANwTN68dhOz9H7iogJsxzPO-BmjBP11mcBrqjiR4iPZpK3-pQK5MMtuhFlr9NO6SigiaLbr30L8ELORgJWUrXlSz9eyaWGfNq9dcypYFKLw3Sp9aieMNiZBWnSu0w5JlDlx81a5Y3m4aR7CD77V__ZFDH7LkwbeKZpqmize8_srSYW1RIR_B_xzf9tk5Yhb7em7AtQGuV3_KI1yGMDvfdsEJ_K5JVVFzj955G16fJcrlSATfhWFeVnUhgOuflpqWC40hiofFLFfGRvU4TUyjc96QA1EePXrZbLMz_NNZzg_2frN_XOR4j8WM6HwyUl-v-g0W_jHkF_FxoJKGpDWB5al8V_V4WhqdOYUszkH6QeIw8gcy4kqOGfY2xMSrcJNKZlvgBmN37xvOL3zahVn9YTJ2JY3Ydu0S9JRdWHaEkw8A1LskVDcBu0WM_AgaooNmRBfSuPCeij9l8tEsBRfxAnhohp-3I25-3o3VbvSLOKs84hwzArMZct-szOGnOS6Osw9r-YY-oOWiUtBAqQ0LasF0HAS84_u9v9QWEPSqygLnikfIm9XsemLQTt89ADsCQafpOb412awkquSGkqkcpjI7hpBWIYELeSYDTKBVaNL3ErVyBbHlkvfLz7h-FCn2vlmCnin8I78CA3o1gWJtoOz1Phl6zce0KppbYFLmpINR0hwxeozZaR7PRfjJvV8UGgHbP993iQ8IFEJsnCaPrH8ZMRlvd8tuaH9KPfDb6I6ZqQD9fr7xpLPJaexXhk2qA7rDEm3E6KQ3fitZpPz9K9xkDayUFrlzluIH21d91srjNSwK0vBb6Pmj5xgKbaW3isLIDnEMwEfRZUoA02OO69eaGU8WRAp-w../download [1115393/1115393] -> \"input.txt\" [1]\n",
      "-------------Sample text---------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print(\"-------------Sample text---------------\")\n",
    "    print (read_data[0:500])\n",
    "    print(\"---------------------------------------\")\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Data loader</h3>\n",
    "You need to read the input file and convert each character to numerical values. The following cell is a class that helps to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>We have to convert the characters in the string to numbers. Also we need to represent each sequence of characters as a vector in each batch.</p>\n",
    "So, let's set some parameters that we need those now for reading the dataset, and later to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 128  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 50 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Now, we can read the data at batches using the <b>TextLoader</b> class. It will convert the characters to numbers, and represent each sequence as a vector in batches:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [39  5  3 ...  0 20  9]\n",
      " [ 0  5  9 ... 19  4 13]\n",
      " ...\n",
      " [ 3 18 18 ...  1  0 23]\n",
      " [ 7  1 23 ... 18  3  7]\n",
      " [47 26 24 ...  0  8  3]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>Notice:</b> In the following cells, we just go through the process of defining each element of the LSTM, and explore the inputs, outputs of each layer. Then, we put all together and run the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>1- Input and Output</h3>\n",
    "In the next cell we just take a look at a sample batch to underestand the data better. Each batch includes the input, <b>x</b>, and the character that we want to predict, <b>y</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =128, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 5,  3,  0, ..., 20,  9, 27],\n",
       "       [ 5,  9, 14, ...,  4, 13, 20],\n",
       "       ...,\n",
       "       [18, 18,  9, ...,  0, 23, 11],\n",
       "       [ 1, 23,  3, ...,  3,  7,  0],\n",
       "       [26, 24, 10, ...,  8,  3,  2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"lstm\"></a>\n",
    "<h3>2- What is Long Short-Term Memory Model (LSTM)?</h3>\n",
    "\n",
    "<p>Recurrent Neural Networks are Deep Learning models with simple structures and a feedback mechanism built-in, or in different words, the output of a layer is added to the next input and fed back to the same layer.</p>\n",
    "\n",
    "<p>The Recurrent Neural Network is a specialized type of Neural Network that solves the issue of <b>maintaining context for Sequential data</b> -- such as Weather data, Stocks, Genes, etc. At each iterative step, the processing unit takes in an input and the current state of the network, and produces an output and a new state that is <b>re-fed into the network</b>.</p>\n",
    "\n",
    "<p>However, <b>this model has some problems</b>. It's very computationally expensive to maintain the state for a large amount of units, even more so over a long amount of time. Additionally, Recurrent Networks are very sensitive to changes in their parameters. To solve these problems, we use a specific type of RNN, is called Long Short-Term Memory (LSTM).</p>\n",
    "\n",
    "\n",
    "Each LSTM cell has 5 parts:\n",
    "<ol>\n",
    "    <li>Input</li>\n",
    "    <li>prv_state</li>\n",
    "    <li>prv_output</li>\n",
    "    <li>new_state</li>\n",
    "    <li>new_output</li>\n",
    "</ol>\n",
    "\n",
    "<ul>\n",
    "    <li>Each LSTM cell has an input layer, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of Word2Vec embedding, for each character.</li>\n",
    "    <li>Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size, in literature.</li>\n",
    "    <li>An LSTM keeps two pieces of information as it propagates through time:</li> \n",
    "    <ul>\n",
    "         <li><b>hidden state</b> vector: Each LSTM cell accept a vector, called <b>hidden state</b> vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The <b>hidden state</b> vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". Number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.</li>\n",
    "        <li><b>previous time-step output</b>: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell.</li> \n",
    "    </ul>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "<h4>Stacked LSTM</h4>\n",
    "<p>What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second.</p>\n",
    "\n",
    "num_layers = 2 \n",
    "<ul>\n",
    "    <li>number of layers in the RNN, is defined by <code>num_layers</code> parameter.</li>\n",
    "    <li>An input of MultiRNNCell is <b>cells</b> which is list of RNNCells that will be composed in this order.</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>3- Defining stacked RNN Cell</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>BasicRNNCell</b> is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to define a LSTM cell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>state</b> variable keeps output and new_state of the LSTM, so it is double in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 128x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "and target data, what we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 128x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "<b>BasicRNNCell.zero_state(batch_size, dtype)</b> Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) \n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session(config=config)\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>4- Embedding</h3>\n",
    "<p>In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix.</p>\n",
    "\n",
    "<p><b>Notice:</b> The function <code>tf.get_variable()</code> is used to share a variable and to initialize it in one place. <code>tf.get_variable()</code> is used to get or create a variable instead of a direct call to <code>tf.Variable</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets take a look at the <b>embedding</b>, <b>em</b>, and <b>inputs</b> variables:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00410442, -0.14476375,  0.05594227, ..., -0.12300719,\n",
       "        -0.10226429, -0.06584811],\n",
       "       [-0.16390008, -0.08364575, -0.02182198, ..., -0.1533403 ,\n",
       "         0.06320992,  0.03918871],\n",
       "       [-0.14483224, -0.03452827, -0.16970718, ..., -0.13157272,\n",
       "         0.02157967, -0.02248065],\n",
       "       ...,\n",
       "       [-0.13904384,  0.0050786 , -0.01849365, ..., -0.16963613,\n",
       "        -0.07352643,  0.15418915],\n",
       "       [-0.06295526,  0.07328682,  0.11295535, ...,  0.01933679,\n",
       "        -0.04166536, -0.09646647],\n",
       "       [ 0.16901128,  0.01937665,  0.09422706, ...,  0.06377672,\n",
       "         0.13484041,  0.13377918]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.04269591,  0.15412812, -0.11515211, ...,  0.15424894,\n",
       "        -0.16588417, -0.01768497],\n",
       "       [ 0.17313541,  0.08467411, -0.17477724, ..., -0.0540156 ,\n",
       "        -0.0787918 ,  0.13528807],\n",
       "       [ 0.03443407,  0.04871383, -0.1293053 , ..., -0.07831013,\n",
       "        -0.04348563, -0.04235049],\n",
       "       ...,\n",
       "       [-0.16390008, -0.08364575, -0.02182198, ..., -0.1533403 ,\n",
       "         0.06320992,  0.03918871],\n",
       "       [-0.13168189, -0.11865976, -0.03168228, ..., -0.07455374,\n",
       "         0.00392222,  0.10635261],\n",
       "       [ 0.03443407,  0.04871383, -0.1293053 , ..., -0.07831013,\n",
       "        -0.04348563, -0.04235049]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Let's consider each sequence as a sentence of length 50 characters, then, the first item in <b>inputs</b> is a [60x128] vector which represents the first characters of 60 sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>5- Feeding a batch of 50 sequence to a RNN:</h3>\n",
    "\n",
    "The feeding process for inputs is as following:\n",
    "<ul>\n",
    "    <li>Step 1: first character of each of the 50 sentences (in a batch) is entered in parallel.</li>  \n",
    "    <li>Step 2: second character of each of the 50 sentences is input in parallel.</li> \n",
    "    <li>Step n: nth character of each of the 50 sentences is input in parallel.</li>  \n",
    "</ul>\n",
    "<p>The parallelism is only for efficiency. Each character in a batch is handled in parallel, but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04269591,  0.15412812, -0.11515211, ...,  0.15424894,\n",
       "        -0.16588417, -0.01768497],\n",
       "       [ 0.14557765, -0.06453255, -0.14769079, ..., -0.09698131,\n",
       "         0.13256927, -0.15082939],\n",
       "       [ 0.00410442, -0.14476375,  0.05594227, ..., -0.12300719,\n",
       "        -0.10226429, -0.06584811],\n",
       "       ...,\n",
       "       [-0.15891185,  0.14149551, -0.06006488, ...,  0.05675514,\n",
       "         0.16080476, -0.03014275],\n",
       "       [ 0.03443407,  0.04871383, -0.1293053 , ..., -0.07831013,\n",
       "        -0.04348563, -0.04235049],\n",
       "       [-0.11665647, -0.17611335, -0.06220502, ...,  0.05356807,\n",
       "         0.03968026, -0.09509702]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07193604,  0.04996102,  0.02819808, ...,  0.07372162,\n",
       "         0.08411679, -0.06137571],\n",
       "       [-0.04214062,  0.04288679,  0.03801601, ...,  0.07990123,\n",
       "        -0.10864237,  0.03566001],\n",
       "       [-0.06456021,  0.00308322, -0.04612709, ...,  0.04802978,\n",
       "         0.04185934,  0.15868655],\n",
       "       ...,\n",
       "       [-0.04538914,  0.02101073,  0.18103151, ..., -0.00617935,\n",
       "        -0.04592714, -0.03190456],\n",
       "       [ 0.0109611 , -0.0294967 ,  0.13075407, ...,  0.00564974,\n",
       "         0.00202648, -0.08741152],\n",
       "       [ 0.00773069,  0.03939327,  0.06650358, ...,  0.07333107,\n",
       "         0.02463407,  0.10821469]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>As it was explained, <b>outputs</b> variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The <b>softmax_w</b> shape is [rnn_size, vocab_size], which is [128x65] in our case. Therefore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the <b>softmax(output * softmax_w + softmax_b)</b> for this purpose. The shape of the matrixis would be:</p>\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(6400, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01412071, 0.01203302, 0.02083781, ..., 0.01389595, 0.01904284,\n",
       "        0.01369697],\n",
       "       [0.01575099, 0.01560969, 0.01794309, ..., 0.01465707, 0.02106142,\n",
       "        0.0149718 ],\n",
       "       [0.010363  , 0.01209324, 0.02042551, ..., 0.01693923, 0.01540113,\n",
       "        0.01348543],\n",
       "       ...,\n",
       "       [0.00849761, 0.01333721, 0.01664877, ..., 0.01570084, 0.01089002,\n",
       "        0.0118237 ],\n",
       "       [0.0133075 , 0.01219861, 0.01736959, ..., 0.01013201, 0.02355772,\n",
       "        0.00974784],\n",
       "       [0.01247451, 0.01831656, 0.00983814, ..., 0.01413895, 0.01790724,\n",
       "        0.01109813]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we are in the position to calculate the cost of training with <b>loss function</b>, and keep feeding the network to learn it. But, the question is: what does the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Okay, by now, you should understand enough about each component of a LSTM network to be able to train it, and predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>All together</h2>\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False, device='/cpu:0'):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 128 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        with tf.device(device):\n",
    "            # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "            basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "            # model.cell.state_size is (128, 128)\n",
    "            self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "            self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "            self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "            # Initial state of the LSTM memory.\n",
    "            # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "            self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "            with tf.variable_scope('rnnlm_class1'):\n",
    "                softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "                softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs] \n",
    "\n",
    "            # The value of state is updated after processing each batch of chars.\n",
    "            outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "            output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "            self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                    [tf.reshape(self.targets, [-1])],\n",
    "                    [tf.ones([batch_size * seq_length])],\n",
    "                    vocab_size)\n",
    "            self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "            self.final_state = last_state\n",
    "            self.lr = tf.Variable(0.0, trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"cpu_vs_gpu\"></a>\n",
    "<h2>Train your model using CPU and GPU</h2>\n",
    "We can train our model through feeding batches. You should be able to complete the following cells and submit it for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_1\"></a>\n",
    "<h2>Question 1: Complete the code to run it on CPU</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.039, time/batch = 0.063\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The myow ci, raw id enithzer brordt, call verares; heare:\n",
      "Theren in sit hto.\n",
      "Whervounded no ntanus gastese'd I buse shen fai thou st\n",
      "Is falline to Mastone\n",
      "----------------------------------\n",
      "347/8700 (epoch 1), train_loss = 1.840, time/batch = 0.060\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The are is last,\n",
      "Sling a mindor.\n",
      "\n",
      "IUPrENIN\n",
      "My crichund ellow.\n",
      "\n",
      "LAUM:\n",
      "I this him nother knowh a shallt to tree soricknow no whencan on, in my seaded duce y\n",
      "----------------------------------\n",
      "521/8700 (epoch 2), train_loss = 1.737, time/batch = 0.053\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The twitcas and me know not lownots, wicked you or sake but you\n",
      "Tresuch sith a may it oursh gature\n",
      "Wene; for were a too grudies Have Sones ham.\n",
      "\n",
      "GREED:\n",
      "I \n",
      "----------------------------------\n",
      "695/8700 (epoch 3), train_loss = 1.671, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prettrated stor weer willis curn thouse our cruple eacul?\n",
      "Who smonend, not be tol unstry gentle of should lique. wo bast and wration,\n",
      "Iw ourncule.\n",
      "\n",
      "KI\n",
      "----------------------------------\n",
      "869/8700 (epoch 4), train_loss = 1.631, time/batch = 0.052\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wings, your nawcriel we can a sence whree,\n",
      "Clapps bymald.\n",
      "\n",
      "WARPETO:\n",
      "From: though or thy bling is huse my stike sithins what comolest to, caucius, your\n",
      "----------------------------------\n",
      "1043/8700 (epoch 5), train_loss = 1.602, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The oljeads: Mas truke Duke to dorrow revisters? but, this not much cralive glascess, all taughter,\n",
      "If this see, my gord:\n",
      "For this sway,\n",
      "What I guvence.\n",
      "Y\n",
      "----------------------------------\n",
      "1217/8700 (epoch 6), train_loss = 1.579, time/batch = 0.052\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The took aclises;\n",
      "Hourtenals.\n",
      "Sir, I weep thronf'st swear her keke--\n",
      "Or pour causo alpaigurold, I bost, you go; for a creath.\n",
      "\n",
      "POLIXENES:\n",
      "Thy sqreithel,\n",
      "c\n",
      "----------------------------------\n",
      "1391/8700 (epoch 7), train_loss = 1.557, time/batch = 0.052\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The son, by you fingly\n",
      "gles here is both his kervy take alach'd\n",
      "Deserved done from actike me saplent. Here Saigun, thy nothing bering,\n",
      "Measurio; liage tim\n",
      "----------------------------------\n",
      "1565/8700 (epoch 8), train_loss = 1.541, time/batch = 0.053\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The bear of a termont:\n",
      "Make, his you most look, my fous od a seen I\n",
      "consent of that he with they for let my from some of Conjest, which and tenger.\n",
      "\n",
      "Secon\n",
      "----------------------------------\n",
      "1739/8700 (epoch 9), train_loss = 1.526, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The taals--\n",
      "Where? Never o'ch they humselp cas thy goods and an\n",
      "To be that play again.\n",
      "\n",
      "HASNurgo:\n",
      "\n",
      "HENRY VIWARD:\n",
      "And you have best not in now, becteding?\n",
      "\n",
      "----------------------------------\n",
      "1913/8700 (epoch 10), train_loss = 1.513, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The with her, seme his queen,\n",
      "That that with a want?\n",
      "\n",
      "POMPEY:\n",
      "But endecy blay, and assure sla\n",
      "Voling her trow?\n",
      "I' the are with them to to; a cird you wher\n",
      "----------------------------------\n",
      "2087/8700 (epoch 11), train_loss = 1.503, time/batch = 0.047\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Voloor:\n",
      "I'll been mother most they horest the fail bay,\n",
      "So your you pirase;\n",
      "The swort it.\n",
      "\n",
      "ANTONIO:\n",
      "It he\n",
      "cready amens hours\n",
      "that reing,\n",
      "Come gown;\n",
      "Su\n",
      "----------------------------------\n",
      "2261/8700 (epoch 12), train_loss = 1.493, time/batch = 0.085\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wash me fit laughters'd old.\n",
      "\n",
      "Second Gentlemen?\n",
      "\n",
      "Nury Deacks! Cours?\n",
      "\n",
      "MORINA:.\n",
      "\n",
      "CLIFFFROMENS:\n",
      "Then he that he may we man:\n",
      "But the looke thy devives in\n",
      "----------------------------------\n",
      "2435/8700 (epoch 13), train_loss = 1.486, time/batch = 0.052\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The that the ground, bow this; thou honour you wishing his speech,\n",
      "Amerseman that a pleasure.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Geath wink, no lit, whech asson send, and I\n",
      "----------------------------------\n",
      "2609/8700 (epoch 14), train_loss = 1.478, time/batch = 0.163\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The gallo,\n",
      "Nay, whree it need play buspoon:\n",
      "And metosclable, flace him: what eyes, the dempropy accept\n",
      "coprod where felto with own the wantiful satistain \n",
      "----------------------------------\n",
      "2783/8700 (epoch 15), train_loss = 1.472, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The caps,\n",
      "Histers to says no; sit strike usurroy, I thy mounted of his widine,\n",
      "And I must that agot that appoys,\n",
      "And me s! thou hast see, the farment\n",
      "And \n",
      "----------------------------------\n",
      "2957/8700 (epoch 16), train_loss = 1.466, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The matter, thereford cople?\n",
      "Unless that dreabs chasper to speak Bichove trembring forture.\n",
      "'Tis to fair I pray you,\n",
      "I will be will\n",
      "The france may behest.\n",
      "----------------------------------\n",
      "3131/8700 (epoch 17), train_loss = 1.462, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The but in queen as Else?\n",
      "\n",
      "Nursh:\n",
      "Marry, nothing I, and stay every fortune enouple.\n",
      "\n",
      "LUCIO:\n",
      "It is becombay of of condemine: you be-strance, and afterct; t\n",
      "----------------------------------\n",
      "3305/8700 (epoch 18), train_loss = 1.458, time/batch = 0.049\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The death!\n",
      "\n",
      "BRUTUS:\n",
      "Who would rive\n",
      "Of my king her knows, Pacumore it!\n",
      "Come known to soul;\n",
      "To one can I to give a floth mound his ere it. But, good ming fr\n",
      "----------------------------------\n",
      "3479/8700 (epoch 19), train_loss = 1.454, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The quarry dollecting\n",
      "a but\n",
      "I think all thee, complain'd the man you; it stay by the chance by.\n",
      "\n",
      "GREMIO:\n",
      "I should did broth on\n",
      "Clasding of this tear to se\n",
      "----------------------------------\n",
      "3653/8700 (epoch 20), train_loss = 1.451, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The hander to the matter.\n",
      "\n",
      "CORIOLANUS:\n",
      "Let that the davet till her wrong, marriage, forgois me?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "No, plamence, foolish to to,\n",
      "You than hi\n",
      "----------------------------------\n",
      "3827/8700 (epoch 21), train_loss = 1.447, time/batch = 0.052\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The father was being return soul contight; where that I would be like it day;\n",
      "Why, pray, a pit.\n",
      "\n",
      "First: I ambrust, mine.\n",
      "\n",
      "CATESBY:\n",
      "I make great This his p\n",
      "----------------------------------\n",
      "4001/8700 (epoch 22), train_loss = 1.444, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The worsh,\n",
      "To waxe, or this nland,\n",
      "And his heavt wandias,\n",
      "Desimited actions waterint in Jaim.\n",
      "So fay so the daught souls arity:\n",
      "Your are them kecily. That\n",
      "----------------------------------\n",
      "4175/8700 (epoch 23), train_loss = 1.441, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The safe.\n",
      "\n",
      "GONZALO:\n",
      "Why I'll such the walls, thou hasts his frilive his peril light posing: the should feiln all but ingrand oa orar plower of hims, makin\n",
      "----------------------------------\n",
      "4349/8700 (epoch 24), train_loss = 1.438, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dog, a all\n",
      "We's block.\n",
      "Thou crumber and how, sir: pale-want,\n",
      "Well fellows mostward forth you rademy:\n",
      "The many make should assug his man! painty, gentl\n",
      "----------------------------------\n",
      "4523/8700 (epoch 25), train_loss = 1.435, time/batch = 0.053\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The furch. We!' fare of thing, but heaving at that, So show the smy'st that her abodour,\n",
      "Rather?\n",
      "But that thy man's dukble your very so know not shape in \n",
      "----------------------------------\n",
      "4697/8700 (epoch 26), train_loss = 1.432, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dance;\n",
      "Thou thunder to give me not\n",
      "And you may death weeph in the tidul'd son;\n",
      "I will be rome in you.\n",
      "\n",
      "ROMEO:\n",
      "Let in sometimn would upon his sack to t\n",
      "----------------------------------\n",
      "4871/8700 (epoch 27), train_loss = 1.430, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The moon, and what now the pardon him committy;\n",
      "The temp, to all take that she Clibjured the ear--your last, I have many with the foum 'twards have the se\n",
      "----------------------------------\n",
      "5045/8700 (epoch 28), train_loss = 1.427, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The say with him?\n",
      "Marament the ears keeper, sir: in rude your sto't piled ours: who like the command, his name that's onlong. I do not his naturath this h\n",
      "----------------------------------\n",
      "5219/8700 (epoch 29), train_loss = 1.425, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The strengle done,\n",
      "the wine they bound age, I here gone: the trimms within the jester of witests:\n",
      "I for I speak me, northy to this cusiciar still thought \n",
      "----------------------------------\n",
      "5393/8700 (epoch 30), train_loss = 1.423, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The slaw, thy man's flather; day thouse not his husband.\n",
      "\n",
      "FRIAN:\n",
      "Now not.\n",
      "\n",
      "BRUTUS:\n",
      "Why, the birth, is Chip her face in joat of I speak,\n",
      "And spection; to m\n",
      "----------------------------------\n",
      "5567/8700 (epoch 31), train_loss = 1.421, time/batch = 0.048\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The compan regod were encoudles whore or reages no down.\n",
      "\n",
      "Second Herefull usek, anchorps;\n",
      "that cause you shalt look to this nextays:\n",
      "What stone of heaven,\n",
      "----------------------------------\n",
      "5741/8700 (epoch 32), train_loss = 1.419, time/batch = 0.053\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sadch, and enjoy laids, with whatoe, he weal go so grilpe sound sail, as the strough'd more; your husband'st\n",
      "Aman\n",
      "And I have as wains upon true;\n",
      "And d\n",
      "----------------------------------\n",
      "5915/8700 (epoch 33), train_loss = 1.417, time/batch = 0.049\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sorrow. Hoe you asy briegred, and heard a wore\n",
      "To being all age shall I chose were to want's thum of bim impostain.\n",
      "\n",
      "CLARENCE:\n",
      "I must the sheling verg\n",
      "----------------------------------\n",
      "6089/8700 (epoch 34), train_loss = 1.415, time/batch = 0.074\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The king of dirucre them!\n",
      "\n",
      "KING RICHARD MARLAND:\n",
      "Ene a urgainsage to the heads,\n",
      "As elder, I was a bawning unexais.\n",
      "Where I am with the pitiend with\n",
      "To acc\n",
      "----------------------------------\n",
      "6263/8700 (epoch 35), train_loss = 1.414, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The harre.\n",
      "\n",
      "Secendituest dowr desire.\n",
      "The daughter\n",
      "Thereot blames! That God doth villain ristancer,\n",
      "Which thou to thy abfuty cousin,\n",
      "If thou art think und\n",
      "----------------------------------\n",
      "6437/8700 (epoch 36), train_loss = 1.412, time/batch = 0.058\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The crabul to the apose;\n",
      "But not we very both.\n",
      "\n",
      "Nuch-Anutaments, uncles honest, which give his team me Let?\n",
      "\n",
      "LEONTES:\n",
      "Precate:\n",
      "The good: how one incerity \n",
      "----------------------------------\n",
      "6611/8700 (epoch 37), train_loss = 1.411, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The cice,\n",
      "The town'd?\n",
      "\n",
      "SThon:\n",
      "Here Jove haply\n",
      "Monstry their dukech unless with this nuarward's full, the patienled change\n",
      "is this is yet yet and broken ch\n",
      "----------------------------------\n",
      "6785/8700 (epoch 38), train_loss = 1.409, time/batch = 0.050\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The three-hearts shake is me;\n",
      "And, he brother your formasker\n",
      "Shall have strong up the honoury his fire him.\n",
      "Thou hath severain a call\n",
      "That your tender of \n",
      "----------------------------------\n",
      "6959/8700 (epoch 39), train_loss = 1.408, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The goher'd resolt within.\n",
      "Vount is a cleasure encess.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Muster thy clower,\n",
      "Forgew their days,\n",
      "But ss all my furterlauf to Larger that new\n",
      "----------------------------------\n",
      "7133/8700 (epoch 40), train_loss = 1.406, time/batch = 0.052\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Duke, and by so command; as the being the appates\n",
      "Which Tranio hold of prayers.\n",
      "\n",
      "HORTENSIO:\n",
      "Haply so happoir's teay\n",
      "To prove and time speak;\n",
      "That, or,\n",
      "----------------------------------\n",
      "7307/8700 (epoch 41), train_loss = 1.405, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The is childer.\n",
      "\n",
      "POLIXENES:\n",
      "As--boot her sup,\n",
      "Sir state.\n",
      "I hath holds ladgest, die;\n",
      "Which his tribions, think just\n",
      "Who bewroot it is rexamen quonce.\n",
      "\n",
      "Clow\n",
      "----------------------------------\n",
      "7481/8700 (epoch 42), train_loss = 1.404, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The time:\n",
      "I say ushall bown who,\n",
      "Such as take; and retule\n",
      "Sir, to deavens, as some puts have short, with his to-name, this, not here a never dream?\n",
      "\n",
      "NORTH\n",
      "----------------------------------\n",
      "7655/8700 (epoch 43), train_loss = 1.402, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The a sigger, it is:\n",
      "He what, thy sure full you; rad with air: time is the proves;\n",
      "And from, he shall grace do besite: there had aw to this married.\n",
      "\n",
      "DUKE\n",
      "----------------------------------\n",
      "7829/8700 (epoch 44), train_loss = 1.401, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The court?\n",
      "I'll standed not honest thing we would for her, lord?\n",
      "\n",
      "AUFIBARET:\n",
      "Soudain's traw,\n",
      "Which; I yarry you, you would know lines to-moot his sorrow, \n",
      "----------------------------------\n",
      "8003/8700 (epoch 45), train_loss = 1.400, time/batch = 0.053\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pack'd as, of your Katharoy:\n",
      "Hay, in the wine\n",
      "Bicla, mighty the charge me,\n",
      "That import now on the sap home!\n",
      "I bear that way his tong and gentluble,\n",
      "An\n",
      "----------------------------------\n",
      "8177/8700 (epoch 46), train_loss = 1.399, time/batch = 0.057\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The time many dead, good chasing lies:\n",
      "As Ellence upon errip; vrince the gand\n",
      "The good wither alhous wisoence.\n",
      "\n",
      "JULIET:\n",
      "O thought weeph, high walkerous un\n",
      "----------------------------------\n",
      "8351/8700 (epoch 47), train_loss = 1.398, time/batch = 0.051\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pregothior.\n",
      "Petitionso sex I remender:\n",
      "Speak, means bid mort my decelvet.\n",
      "\n",
      "Clops je:\n",
      "Thou liege ensuris. Since all the from, proclaim: here I was more\n",
      "----------------------------------\n",
      "8525/8700 (epoch 48), train_loss = 1.397, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The doing,\n",
      "Ambsteater to Pady.\n",
      "O Polem arait known, villaken inst oaking with a reputer\n",
      "Time Aumemans I hear in them we agaib\n",
      "Abbights bridly, I may be st\n",
      "----------------------------------\n",
      "8699/8700 (epoch 49), train_loss = 1.396, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dark but by the femmoorly;\n",
      "Houre Athiel's whither what tita want thou born, and to childress to my sixters have-wall full say,\n",
      "I woe we bestmer in our\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "avg_batch_running_duration_CPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn_CPU\"):\n",
    "    model = LSTMModel(device='/cpu:0')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        ## write your code bellow to reset the batch pointer in data_loader. you can use reset_batch_pointer()\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        state = sess.run(model.initial_state) # model initialization\n",
    "        batch_running_duration_CPU = []\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            ## write your code to define your x and y. You should use next_batch() from data_loader\n",
    "            x,y = data_loader.next_batch()\n",
    "            \n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            \n",
    "            ## write your code to train the model\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "\n",
    "            end = time.time()\n",
    "            \n",
    "            ## write your code to store the duration of runing each batch in a list (end - start)\n",
    "            batch_running_duration_CPU.append(end - start)\n",
    "            \n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        avg_batch_running_duration_CPU.append(sum(batch_running_duration_CPU) / float(len(batch_running_duration_CPU)))\n",
    "        \n",
    "        # Please uncomment the following block of the code so the grader can see the sample of prediction\n",
    "        with tf.variable_scope(\"rnn_CPU\", reuse=True):\n",
    "             sample_model = LSTMModel(sample=True)\n",
    "             print ('----------------------------------')\n",
    "             print ('SAMPLE GENERATED TEXT:')\n",
    "             print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "             print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_2\"></a>\n",
    "<h2>Question 2: Complete the code to run it on GPU</h2>\n",
    "Now, create the same network with GPU, and calculate the time/batch for running each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.058, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The beit say Whot d derphee rave tuly gold me\n",
      "For eid,\n",
      "Het thee the calfre fails:\n",
      "I woled bowith Wime trem;\n",
      "To wath we.\n",
      "\n",
      "HEMANTIA:\n",
      "His the row'y alle,\n",
      "Sae\n",
      "----------------------------------\n",
      "347/8700 (epoch 1), train_loss = 1.843, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The have to\n",
      "Harle? what fare I\n",
      "Rave serpen nut you wouct downot my liked, wall ency;\n",
      "Mander have.\n",
      "\n",
      "First Gonger\n",
      "Tifers, this of the goak the on mareal, no\n",
      "----------------------------------\n",
      "521/8700 (epoch 2), train_loss = 1.732, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sear'sie, wiser, I care part the wated.\n",
      "\n",
      "LUCHORI:\n",
      "When mine here bid Strein'd:\n",
      "A glangek lapn't the ouf fat, distions aspleatiob's son thy good porfit\n",
      "----------------------------------\n",
      "695/8700 (epoch 3), train_loss = 1.667, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sance:\n",
      "Rheeble this hath leainifor'd thee itper on thy long?\n",
      "\n",
      "Prond Puster.\n",
      "\n",
      "MONANLE:\n",
      "Before charfulch'dl way: sitmen stors\n",
      "Kich Warchaked the fuls, p\n",
      "----------------------------------\n",
      "869/8700 (epoch 4), train_loss = 1.626, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The stand she\n",
      "The must dellous kway,\n",
      "He are all are lover werries: how say, whose let setia,\n",
      "and I will woelds\n",
      "he arword king,\n",
      "Yet insway shasty when it!\n",
      "\n",
      "----------------------------------\n",
      "1043/8700 (epoch 5), train_loss = 1.597, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Evouch, like Contonan me lep-then Postard!\n",
      "Wish slefise all his for but, them say, of sir; whet an?\n",
      "Low in accease, should cale: for mere these for in\n",
      "----------------------------------\n",
      "1217/8700 (epoch 6), train_loss = 1.575, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The besting to be Lond, my Northy,\n",
      "When I womnot this Duke; thou foul here is gace again, wiGh:\n",
      "O, Well rin offence eyes,\n",
      "Nazed with to this charith'\n",
      "for \n",
      "----------------------------------\n",
      "1391/8700 (epoch 7), train_loss = 1.556, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The spirit.\n",
      "\n",
      "ELERD:\n",
      "Take you lovant.\n",
      "\n",
      "GOSZALO:\n",
      "Here, I horn, but berus, I lietemon of his disp,\n",
      "And now love, my is heavy angets,\n",
      "Some to dith, sir, sa su\n",
      "----------------------------------\n",
      "1565/8700 (epoch 8), train_loss = 1.541, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The chysuch us back his birs's master.\n",
      "\n",
      "CAFfOUD:\n",
      "Marcutier:\n",
      "Sign the amseevicgle settle Fivesty a!\n",
      "\n",
      "ROTHORIZANSABESSANLO:\n",
      "This this aid;\n",
      "thy thum, you whi\n",
      "----------------------------------\n",
      "1739/8700 (epoch 9), train_loss = 1.529, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wring delivates sourn?\n",
      "\n",
      "Second Cavercide with was and farwards,\n",
      "Not can men in, hoperaltake tender'als,\n",
      "He is know with it.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Then, swe\n",
      "----------------------------------\n",
      "1913/8700 (epoch 10), train_loss = 1.518, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sheart but the\n",
      "now hell; anlend of a waBron:\n",
      "Shall be serveeds:\n",
      "'ill, myal you is not somety wail you planenned, thou seezardons me\n",
      "can sir?\n",
      "\n",
      "Lorvest \n",
      "----------------------------------\n",
      "2087/8700 (epoch 11), train_loss = 1.508, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The one sufflust. Gers are kiss Clarence me\n",
      "Manibn, a feason,\n",
      "That should the Cyessed, laferm'd this;\n",
      "And him:\n",
      "Made is he sidles of fells of with our hear\n",
      "----------------------------------\n",
      "2261/8700 (epoch 12), train_loss = 1.498, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The lock strain hands\n",
      "Than I too you the hage hanst all the faint, I pray weed the raigs?\n",
      "\n",
      "Mistaude,\n",
      "Here instle mar: imply at the mistle, I be no leatie!\n",
      "----------------------------------\n",
      "2435/8700 (epoch 13), train_loss = 1.491, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The of this since men:\n",
      "Seell farili's becoun thing him least. Who wine that Marcius done, as this counly.\n",
      "Which, do youngging stesps of you yet well devie\n",
      "----------------------------------\n",
      "2609/8700 (epoch 14), train_loss = 1.483, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The else, you hangs, I am accasied.\n",
      "\n",
      "ThARI EDINAN:\n",
      "A sweer betcon,\n",
      "For who's his ere I gnor word thou sam:\n",
      "wiselance\n",
      "That stretam:\n",
      "And ansion, down instru\n",
      "----------------------------------\n",
      "2783/8700 (epoch 15), train_loss = 1.477, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The imms of made here see?\n",
      "KI that, in no fieces? browD, till the cable the true worse, 'Slake om you, but with merch free\n",
      "spain, be well in the montawior\n",
      "----------------------------------\n",
      "2957/8700 (epoch 16), train_loss = 1.472, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The provys, those he' man\n",
      "Frour loadins\n",
      "oad inkly follown three met?\n",
      "Give\n",
      "she send to Edward warried otherar and good killiasw it: toob God not,\n",
      "Withing\n",
      "W\n",
      "----------------------------------\n",
      "3131/8700 (epoch 17), train_loss = 1.466, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pass of Cair'd communders thoice. How uparity,\n",
      "OF I think you firsting is stilrred-for but and this hus true still our sursee, briat, no;\n",
      "Let on her s\n",
      "----------------------------------\n",
      "3305/8700 (epoch 18), train_loss = 1.462, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The down tell that no appearable as swords to me is fly sender's gefore me. Thou will\n",
      "That sife ant amble art crian toil suitingly gentle comfortule feay;\n",
      "----------------------------------\n",
      "3479/8700 (epoch 19), train_loss = 1.457, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The could lack when you breathed for at like equers 'ty touse and that all, thou art thou consenio, strike.\n",
      "An this no mornament\n",
      "Shall be nagen\n",
      "To take.\n",
      "\n",
      "\n",
      "----------------------------------\n",
      "3653/8700 (epoch 20), train_loss = 1.453, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The grieve for? farecutcheming-fair,\n",
      "Onst, All dokemping to Chod-sweried a membred.\n",
      "\n",
      "KING RICHARD III:\n",
      "Host thou lold crisamen upon the immety:\n",
      "I would be\n",
      "----------------------------------\n",
      "3827/8700 (epoch 21), train_loss = 1.450, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The shunns we unwitio, some high the\n",
      "bried.\n",
      "\n",
      "THAMIO:\n",
      "Now these whom hold make,\n",
      "with yest that we perish\n",
      "To do you suit.\n",
      "\n",
      "ADraLS:\n",
      "Deitened,\n",
      "And of no tenve\n",
      "----------------------------------\n",
      "4001/8700 (epoch 22), train_loss = 1.446, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The ear myself Harks, which is cutterge hatter traitor's order; and do; wheread was,\n",
      "But my cauber?\n",
      "\n",
      "First MARINABALLANA:\n",
      "If thold in use falcourse\n",
      "And di\n",
      "----------------------------------\n",
      "4175/8700 (epoch 23), train_loss = 1.443, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Lord! Riceld it is our speat have but\n",
      "He't slatge again,\n",
      "And man on the\n",
      "shourd give you: know her look and carried such give the saw, as Shals an proc\n",
      "----------------------------------\n",
      "4349/8700 (epoch 24), train_loss = 1.440, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The perching herself\n",
      "As your dain.\n",
      "\n",
      "VIRTIA:\n",
      "Come that is it not a proceiver than upon my greater\n",
      "Storn, your prising? come from the orself;\n",
      "So, 'tis Tetes\n",
      "----------------------------------\n",
      "4523/8700 (epoch 25), train_loss = 1.437, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Burestal in the peactian Ansulper?\n",
      "And purple,\n",
      "And if projessity.\n",
      "Why, say it. What say on king's guilty, thy man to him:\n",
      "That have sisted of reamerly\n",
      "----------------------------------\n",
      "4697/8700 (epoch 26), train_loss = 1.434, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Lord Clife so coward with your ground, Nondam, thy a said and woman's a wife.\n",
      "\n",
      "KING RICHARD II:\n",
      "Dost indes a wonder which\n",
      "Better, sir, I will restilix\n",
      "----------------------------------\n",
      "4871/8700 (epoch 27), train_loss = 1.432, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The enemal, very come; les it fire all the asks we no browf against come in my noble lack out this is a duke to between will unnock?\n",
      "They go Ralond:\n",
      "What \n",
      "----------------------------------\n",
      "5045/8700 (epoch 28), train_loss = 1.430, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The counts\n",
      "Make fairer will made\n",
      "The can\n",
      "or the deputy, my bringings that glow him two be you have cheer of who, good Lady he are.\n",
      "\n",
      "THARINA:\n",
      "But thou crio\n",
      "----------------------------------\n",
      "5219/8700 (epoch 29), train_loss = 1.428, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prise thone of my spend\n",
      "Do it, and thou excle: thy never:\n",
      "The returname,\n",
      "Op't will betwar,\n",
      "No man's the morning me:,\n",
      "I best nussious mere soulgent;\n",
      "Pe\n",
      "----------------------------------\n",
      "5393/8700 (epoch 30), train_loss = 1.426, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Cleads.\n",
      "By this Casterous'd; then's not like with Richman:\n",
      "You would deserver night\n",
      "And my savict and left\n",
      "Edward\n",
      "The said:\n",
      "The botty.\n",
      "\n",
      "First Senown d\n",
      "----------------------------------\n",
      "5567/8700 (epoch 31), train_loss = 1.424, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The spend I the\n",
      "the fiert you to do bless no devedve, and wild praims\n",
      "Will nog i' to maked bedten be intrusted and meliver, sweet beral band? merchors\n",
      "Toa\n",
      "----------------------------------\n",
      "5741/8700 (epoch 32), train_loss = 1.422, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The ey,\n",
      "To death in the croop.\n",
      "\n",
      "GREMIO:\n",
      "Ay,\n",
      "Therew grrey,\n",
      "And go to vill in up.\n",
      "\n",
      "LADY GREYCAND:\n",
      "How provock of us,\n",
      "And thou hast dull'n, cur from.\n",
      "\n",
      "GRUS:\n",
      "\n",
      "----------------------------------\n",
      "5915/8700 (epoch 33), train_loss = 1.420, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The You are a children's morrors mees him able speak do subject show's shall subject to her can me\n",
      "He at there\n",
      "good good heart\n",
      "The wept wear them one swor\n",
      "----------------------------------\n",
      "6089/8700 (epoch 34), train_loss = 1.418, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The visit up to Walcleage, sweet of heads and through I can\n",
      "Of sheep the tercumatior ip\n",
      "Mender know\n",
      "Frot from the veninct inese in the child;\n",
      "The all thus\n",
      "----------------------------------\n",
      "6263/8700 (epoch 35), train_loss = 1.416, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The comfort thou with a high her tust thou cusacul annived\n",
      "And see and lord, alone,\n",
      "Or no feams's die:\n",
      "That a come and very thy feact flood; thy sheset, s\n",
      "----------------------------------\n",
      "6437/8700 (epoch 36), train_loss = 1.415, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The supmon, easimp a boot glorene of thy seat\n",
      "To chutter through knowly.\n",
      "\n",
      "HENS RICHARD:\n",
      "Therefore, sir, mine,\n",
      "And Lord Late'er bainted as to Clared to the\n",
      "----------------------------------\n",
      "6611/8700 (epoch 37), train_loss = 1.413, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The provishy will.\n",
      "\n",
      "CORIOLANUS:\n",
      "Yes this boy,\n",
      "Or from me,\n",
      "Canst\n",
      "but love? Betow, thour pale,\n",
      "Is this help of two go 'lse than I live on, sir: say thee, co\n",
      "----------------------------------\n",
      "6785/8700 (epoch 38), train_loss = 1.412, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The way to Warwick.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Ir thunk that that dest death; and this hear, 'twill be prince this toke hones.\n",
      "\n",
      "BRUTUS:\n",
      "Take to Henry, queen! Though\n",
      "----------------------------------\n",
      "6959/8700 (epoch 39), train_loss = 1.410, time/batch = 0.024\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Seriping thee; come ye,\n",
      "I'll not keep or prone thus cursench and breath other:\n",
      "Good more please of theie for I have from nurs valour by world and peop\n",
      "----------------------------------\n",
      "7133/8700 (epoch 40), train_loss = 1.409, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The waler-wittens the siful juken\n",
      "to thy charger'd that and think none and her he grloked: I seem through,\n",
      "Af you are to bid hardward-pips I let the fault\n",
      "----------------------------------\n",
      "7307/8700 (epoch 41), train_loss = 1.408, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The giest!\n",
      "\n",
      "Second Citizen:\n",
      "What arm then not to join an horsed;\n",
      "But I cassas unwign,\n",
      "May to advender god\n",
      "And two'gee where your jay a virtue, we stoor', \n",
      "----------------------------------\n",
      "7481/8700 (epoch 42), train_loss = 1.407, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Sorford.\n",
      "\n",
      "PAULINA:\n",
      "Stand stand.\n",
      "\n",
      "KING HENRY VI:\n",
      "Wes now their hot ill:\n",
      "Think, yas a' hups sime?\n",
      "\n",
      "PRONIO:\n",
      "What he danched stand his veck office of a ke\n",
      "----------------------------------\n",
      "7655/8700 (epoch 43), train_loss = 1.406, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The knowled made;\n",
      "Sa cry the flatter.'\n",
      "\n",
      "NABEOL:\n",
      "O God, good Vienns sun, as thou wilt the word uncle; he be the crown, down, I am ne too almorses.\n",
      "\n",
      "COMINIU\n",
      "----------------------------------\n",
      "7829/8700 (epoch 44), train_loss = 1.405, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The had nor east thou art be craves good night.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Is soubs,\n",
      "Though, you sir.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Let me good for blessed stand else all;\n",
      "Witn,\n",
      "Who \n",
      "----------------------------------\n",
      "8003/8700 (epoch 45), train_loss = 1.404, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The eyes of him.\n",
      "Sir great a but that with sweeting me.\n",
      "\n",
      "First Citizen:\n",
      "Constay with the chorted with Affected,\n",
      "I drook my mother that is chastily, this u\n",
      "----------------------------------\n",
      "8177/8700 (epoch 46), train_loss = 1.403, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The gods, the man all do cold me:\n",
      "You know thone. go king, for no\n",
      "man well.\n",
      "\n",
      "Thray shall you the blood sloudes to these seem.\n",
      "\n",
      "First Gentlood! I had, then\n",
      "----------------------------------\n",
      "8351/8700 (epoch 47), train_loss = 1.402, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prise upon? Therefore on the duke by your minds, we comes he so farding me;\n",
      "And, as you do saluiry to your hands,\n",
      "That the sing, above her.\n",
      "\n",
      "PENDUS:\n",
      "I\n",
      "----------------------------------\n",
      "8525/8700 (epoch 48), train_loss = 1.401, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prove dead.\n",
      "\n",
      "LUCENTIO:\n",
      "What arother I say, I forth, the nameny light ye--and she,\n",
      "And he look'd. How well,\n",
      "And thou turnop that to crown\n",
      "for I\n",
      "cut yea\n",
      "----------------------------------\n",
      "8699/8700 (epoch 49), train_loss = 1.401, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Venice have nish need; and away you.\n",
      "\n",
      "LORD LIURHEN:\n",
      "Then, flatterly?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "'Twere and heard, for thee: Tybe's farewelp towe,\n",
      "The one under \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Create the same network using GPU\n",
    "\n",
    "\n",
    "avg_batch_running_duration_GPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn_GPU\"):\n",
    "    model = LSTMModel(device='/gpu:0')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        ## write your code bellow to reset the batch pointer in data_loader. you can use reset_batch_pointer()\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        state = sess.run(model.initial_state) # model initialization\n",
    "        batch_running_duration_GPU = []\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            ## write your code to define your x and y. You should use next_batch() from data_loader\n",
    "            x,y = data_loader.next_batch()\n",
    "\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}            \n",
    "            \n",
    "            ## write your code to train the model\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "\n",
    "            end = time.time()\n",
    "            \n",
    "            ## write your code to store the duration of runing each batch in a list (end - start)\n",
    "            batch_running_duration_GPU.append(end - start)\n",
    "\n",
    "            \n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        avg_batch_running_duration_GPU.append(sum(batch_running_duration_GPU) / float(len(batch_running_duration_GPU)))\n",
    "        \n",
    "        # Please uncomment the following block of the code so the grader can see the sample of prediction\n",
    "        with tf.variable_scope(\"rnn_GPU\", reuse=True):\n",
    "             sample_model = LSTMModel(sample=True)\n",
    "             print ('----------------------------------')\n",
    "             print ('SAMPLE GENERATED TEXT:')\n",
    "             print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "             print ('----------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_3\"></a>\n",
    "<h2>Question 3: Compare the results</h2>\n",
    "Finally, using a graph, show the speed of training (batch/time) for the model running on GPU and CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Epoch')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your code here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(avg_batch_running_duration_CPU, 'o-')\n",
    "plt.plot(avg_batch_running_duration_GPU, 'o-')\n",
    "plt.ylabel('Time')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>Do you want to use GPU in production?</h2>\n",
    "\n",
    "<p>Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The <a href=\"https://cocl.us/ML0122EN_IBMCLOUD_PowerAI\">PowerAI platform on IBM Cloud</a> supports popular machine learning libraries and dependencies including TensorFlow, Caffe, PyTorch, and Theano.</p>\n",
    "\n",
    "<h3>Thanks for completing this lesson!</h3>\n",
    "\n",
    "\n",
    "\n",
    "<h4>Author:  <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>,   <a href=\"https://www.linkedin.com/in/yi-leng-yao-84451275/\">Yi leng Yao</a></h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients’ ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "<p>Copyright &copy; 2018 <a href=\"https://cocl.us/DX0108EN_CC\">Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
