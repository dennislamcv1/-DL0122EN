{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png\" width=\"400px\" align=\"center\"></a>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">Project: Character Modeling</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<font size=\"3\"><strong>In this notebook you will use TensorFlow to create a Recurrent Neural Network, to predict the next character in a string. You need to train your network using a CPU and using a GPU and benchmark the result to see which which device You have to write your code in empty cells in this notebook to make it complete, and then submit the notebook for peer-review.</strong></font>\n",
    "\n",
    "<h2>Table of Contents</h2>\n",
    "<ul>\n",
    "    <li><a href=\"#intro\">Introduction</a></li>\n",
    "    <li><a href=\"#lstm\">Long Short-Term Memory Model (LSTM) Architectures</a></li>\n",
    "    <li><a href=\"#cpu_vs_gpu\">Train your model using CPU and GPU</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#question_1\">Question 1: Complete the code to run it on CPU</a></li>\n",
    "            <li><a href=\"#question_2\">Question 2: Complete the code to run it on GPU</a></li>\n",
    "            <li><a href=\"#question_3\">Question 3: Compare the results</a></li>\n",
    "        </ol>    \n",
    "    </li>\n",
    "</ul>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>This code is supposed to implement a Recurrent Neural Network with LSTM units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.</p>  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "<p>This code is based on this <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">blog</a>, and the code is an step-by-step implementation of the <a href=\"https://github.com/crazydonkey200/tensorflow-char-rnn\">character-level implimentation</a>.</p>\n",
    "\n",
    "<p>I recommend you to complete the \"<a href=\"https://www.edx.org/course/deep-learning-with-tensorflow\">Deep Learning with TensorFlow</a>\" course for this project.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, lets import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Libraries\n",
    "<ul>\n",
    "    <li><b>os</b>: is an module that allows us to interact with the operating systerm, in particular we will use it to set the path in which we will be storing our input file, tensor file and vocab file</li>\n",
    "    <li><b>time</b>: is a library that allows us to access the clock time of our machine, we will use it to measure the performance of training our model with a CPU, versus training our model with a GPU</li>\n",
    "    <li><b>cPickle</b>: is a library for serializing and deserializing python objects, we will use the <b>dump()</b> method in cPickle to serialize our objects when saving them, and <b>load()</b> method in cPickle to deserialize our objects when loading.</li>\n",
    "    <li><b>codec</b>: is a library that deals with character encoding, we will use the <b>open()</b> method as it is recommended when opening encoded text files.</li>\n",
    "    <li><b>collections</b>: is a library that implements high performance container types, we will use the <b>Counter</b> object to get a collection of frequencies for our characters</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Downloading the input data</h3>\n",
    "Lets download the input file, and take a look at some parts of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-03 18:21:38 URL:https://public.boxcloud.com/d/1/b1!SyIRpzayLyWVyV5Agkg6qHmdYgm1nvT70xVrvdo00O2B0VaMrLPJHblEE_ea8bYzTQZRm2Dz8Wr71DD9NYK7MZ1xA1deoDBVxmyIQREugObs9_edbHnObGghumFPCmRnZ2xRjUaXQLSgMoczdYez_lohokiqgkiINoK_JMfId4sQvtWs4mB-wyy2fO3NaHRDeY1omvnuL3MZaabrh-uaaEMcYaQYpXm_RLpSdNnV9mWmsiBnOwJuZMSnlb7i6abB7zs1IuOWXQdcnDishMEA9Z55--82SbJcKc-yeyHTpxpZRU1ZsoOXk0SsSXWkvufLmihqFkldG3cjhZFIHUcj_svcIzIg2WJlQjEAZ6cxa63ATT_eknHA2CjWO_aYV0c_py-VwnbnwMXNjYGxmKsRTe54b9_z1X5EfT4PJkEBBD9L1QxTp3azkuawK9KtA9yhuMm2BDTsBvhNsM8Rb4BI8zvd5jcysF_PhY4q4nxU125tmUPToZFfafNHEmIjqm3HdiI1b7oevxk5XhP09SGC4aSsGpWnnWSEOGfUfcpHKqsp8fSI_pWVn6Kgk-VXcXU3E9vHW3UBVLchf8cSI7llnICiq1-_rdsCkRDWihxPzO9YKhFQRRngCuTbqfmTxhPgjVdjBzC7y8ScUdpEzcTxC43zV8eFfG8YEwq7TVs9kwEyS9ILrzSvV2X4RqbzP-Mc2oN5F7kgV3BKWIpoEX2wsVxN1QlBSf6q4CeurasJyLNPKNtb2wQQggDBZtHw9fYIWW7GZP-K-HWS58CdXdKtgtcvuRJvkUbzCw2FnTVNwAvJM0QoLt0L4Ql5-6M6iBkhW89RInTyH3wekpoDziOrcPSAKBG1kgVZxBQvKE3IM1q845XxPZiEDSU3qkTxLrWEPtmxK9hkZZtXPOlZleT5b-2vEP0KIlH8zMxpOpHvbaM2NMszSdhHlw1FU90Jh9HAmSiO5VaqOYbgA_O68PVSb4JH5CC0vpTQxT5UI0xuxE7MoU8Qp5_tqcPuQr5PBA1mJEV-Jg5TlVnhDu9H2P4B9Uub8kFJHsKg6j-Ywtd19bJCs7uFi1Yc-JGx7KIlLqn9dcnjyOd-IMV2e9-HVaqxJj5n4JbgtFBt6xK_avFJqUl-vsB06Od_0upvAcr1w4vf454lo0_Xs193s62yUhFF_kw-ibe2cgwkGUg-S_S-9SNTYyXMbFl8IUYzSROKmJ11g6awxXu4RLatPyFcmOhd5Od9yGhj2eV8SPvkrHV_rG9k6_z6Qi_DV1-zKzJgIEIK87vOCbtDau3r7THuDAosRlgdCHVbnFuty-N5VuceFrVcbCRahe3SYJQ4vtxZDmIEP1iOFZnsv_964lkr_S9P4FSGlQFbkW88XMMAZwviPQ../download [1115393/1115393] -> \"input.txt\" [1]\n",
      "-------------Sample text---------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print(\"-------------Sample text---------------\")\n",
    "    print (read_data[0:500])\n",
    "    print(\"---------------------------------------\")\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Data loader</h3>\n",
    "You need to read the input file and convert each character to numerical values. The following cell is a class that helps to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>We have to convert the characters in the string to numbers. Also we need to represent each sequence of characters as a vector in each batch.</p>\n",
    "So, let's set some parameters that we need those now for reading the dataset, and later to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 128  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 50 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Now, we can read the data at batches using the <b>TextLoader</b> class. It will convert the characters to numbers, and represent each sequence as a vector in batches:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [39  5  3 ...  0 20  9]\n",
      " [ 0  5  9 ... 19  4 13]\n",
      " ...\n",
      " [ 3 18 18 ...  1  0 23]\n",
      " [ 7  1 23 ... 18  3  7]\n",
      " [47 26 24 ...  0  8  3]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" ,data_loader.vocab_size)\n",
    "print (\"Characters:\" ,data_loader.chars)\n",
    "print (\"vocab number of 'F':\",data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>Notice:</b> In the following cells, we just go through the process of defining each element of the LSTM, and explore the inputs, outputs of each layer. Then, we put all together and run the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>1- Input and Output</h3>\n",
    "In the next cell we just take a look at a sample batch to underestand the data better. Each batch includes the input, <b>x</b>, and the character that we want to predict, <b>y</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =128, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 5,  3,  0, ..., 20,  9, 27],\n",
       "       [ 5,  9, 14, ...,  4, 13, 20],\n",
       "       ...,\n",
       "       [18, 18,  9, ...,  0, 23, 11],\n",
       "       [ 1, 23,  3, ...,  3,  7,  0],\n",
       "       [26, 24, 10, ...,  8,  3,  2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"lstm\"></a>\n",
    "<h3>2- What is Long Short-Term Memory Model (LSTM)?</h3>\n",
    "\n",
    "<p>Recurrent Neural Networks are Deep Learning models with simple structures and a feedback mechanism built-in, or in different words, the output of a layer is added to the next input and fed back to the same layer.</p>\n",
    "\n",
    "<p>The Recurrent Neural Network is a specialized type of Neural Network that solves the issue of <b>maintaining context for Sequential data</b> -- such as Weather data, Stocks, Genes, etc. At each iterative step, the processing unit takes in an input and the current state of the network, and produces an output and a new state that is <b>re-fed into the network</b>.</p>\n",
    "\n",
    "<p>However, <b>this model has some problems</b>. It's very computationally expensive to maintain the state for a large amount of units, even more so over a long amount of time. Additionally, Recurrent Networks are very sensitive to changes in their parameters. To solve these problems, we use a specific type of RNN, is called Long Short-Term Memory (LSTM).</p>\n",
    "\n",
    "\n",
    "Each LSTM cell has 5 parts:\n",
    "<ol>\n",
    "    <li>Input</li>\n",
    "    <li>prv_state</li>\n",
    "    <li>prv_output</li>\n",
    "    <li>new_state</li>\n",
    "    <li>new_output</li>\n",
    "</ol>\n",
    "\n",
    "<ul>\n",
    "    <li>Each LSTM cell has an input layer, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of Word2Vec embedding, for each character.</li>\n",
    "    <li>Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size, in literature.</li>\n",
    "    <li>An LSTM keeps two pieces of information as it propagates through time:</li> \n",
    "    <ul>\n",
    "         <li><b>hidden state</b> vector: Each LSTM cell accept a vector, called <b>hidden state</b> vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The <b>hidden state</b> vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". Number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.</li>\n",
    "        <li><b>previous time-step output</b>: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell.</li> \n",
    "    </ul>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "<h4>Stacked LSTM</h4>\n",
    "<p>What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second.</p>\n",
    "\n",
    "num_layers = 2 \n",
    "<ul>\n",
    "    <li>number of layers in the RNN, is defined by <code>num_layers</code> parameter.</li>\n",
    "    <li>An input of MultiRNNCell is <b>cells</b> which is list of RNNCells that will be composed in this order.</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>3- Defining stacked RNN Cell</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>BasicRNNCell</b> is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to define a LSTM cell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>state</b> variable keeps output and new_state of the LSTM, so it is double in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 128x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "and target data, what we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 128x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "<b>BasicRNNCell.zero_state(batch_size, dtype)</b> Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) \n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session(config=config)\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>4- Embedding</h3>\n",
    "<p>In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix.</p>\n",
    "\n",
    "<p><b>Notice:</b> The function <code>tf.get_variable()</code> is used to share a variable and to initialize it in one place. <code>tf.get_variable()</code> is used to get or create a variable instead of a direct call to <code>tf.Variable</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets take a look at the <b>embedding</b>, <b>em</b>, and <b>inputs</b> variables:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03016365,  0.14466433,  0.0496825 , ..., -0.09817967,\n",
       "         0.11370395,  0.14745824],\n",
       "       [ 0.15353303, -0.02401574, -0.17271619, ..., -0.12104203,\n",
       "        -0.07766873, -0.15579839],\n",
       "       [-0.12415352, -0.00369988,  0.02445474, ...,  0.1303264 ,\n",
       "         0.04497686,  0.03392108],\n",
       "       ...,\n",
       "       [ 0.00917527,  0.11058696,  0.03165144, ...,  0.13692926,\n",
       "         0.16530235,  0.00438447],\n",
       "       [-0.08085521, -0.09708051,  0.01109622, ..., -0.01503989,\n",
       "         0.10171266, -0.1481479 ],\n",
       "       [-0.06195527,  0.12446295,  0.07594304, ...,  0.13734002,\n",
       "         0.1320733 ,  0.06187804]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "#print embedding.shape\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01597753, -0.06804226, -0.1323518 , ...,  0.142866  ,\n",
       "        -0.17332084, -0.00663906],\n",
       "       [ 0.01962219, -0.13072562, -0.05461569, ..., -0.13284838,\n",
       "        -0.0229962 ,  0.141245  ],\n",
       "       [-0.14946897,  0.03115006, -0.09946164, ...,  0.17462997,\n",
       "        -0.08525587, -0.02042338],\n",
       "       ...,\n",
       "       [ 0.15353303, -0.02401574, -0.17271619, ..., -0.12104203,\n",
       "        -0.07766873, -0.15579839],\n",
       "       [-0.12462375, -0.07952023, -0.1184476 , ..., -0.13524987,\n",
       "         0.00898384,  0.00618532],\n",
       "       [-0.14946897,  0.03115006, -0.09946164, ...,  0.17462997,\n",
       "        -0.08525587, -0.02042338]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Let's consider each sequence as a sentence of length 50 characters, then, the first item in <b>inputs</b> is a [60x128] vector which represents the first characters of 60 sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>5- Feeding a batch of 50 sequence to a RNN:</h3>\n",
    "\n",
    "The feeding process for inputs is as following:\n",
    "<ul>\n",
    "    <li>Step 1: first character of each of the 50 sentences (in a batch) is entered in parallel.</li>  \n",
    "    <li>Step 2: second character of each of the 50 sentences is input in parallel.</li> \n",
    "    <li>Step n: nth character of each of the 50 sentences is input in parallel.</li>  \n",
    "</ul>\n",
    "<p>The parallelism is only for efficiency. Each character in a batch is handled in parallel, but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01597753, -0.06804226, -0.1323518 , ...,  0.142866  ,\n",
       "        -0.17332084, -0.00663906],\n",
       "       [-0.00982009, -0.07973815, -0.05742598, ..., -0.10654698,\n",
       "         0.00504619, -0.09834673],\n",
       "       [-0.03016365,  0.14466433,  0.0496825 , ..., -0.09817967,\n",
       "         0.11370395,  0.14745824],\n",
       "       ...,\n",
       "       [ 0.13528122, -0.13439143, -0.00770696, ..., -0.14759111,\n",
       "        -0.07852267, -0.12864742],\n",
       "       [-0.14946897,  0.03115006, -0.09946164, ...,  0.17462997,\n",
       "        -0.08525587, -0.02042338],\n",
       "       [ 0.16482012,  0.13866197,  0.07047909, ...,  0.09593286,\n",
       "        -0.05475925,  0.1308157 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05389741,  0.04110057, -0.10375699, ..., -0.11512218,\n",
       "        -0.01663148, -0.09269854],\n",
       "       [ 0.02005184,  0.1051152 ,  0.09651776, ..., -0.04886765,\n",
       "        -0.0809494 , -0.14422435],\n",
       "       [-0.03081182, -0.18680848,  0.07507442, ...,  0.00394371,\n",
       "         0.12987234, -0.04757503],\n",
       "       ...,\n",
       "       [-0.05252999,  0.1188225 , -0.01286341, ...,  0.03771179,\n",
       "        -0.05435414,  0.02140257],\n",
       "       [-0.0622479 , -0.14971879,  0.04345094, ...,  0.04795458,\n",
       "         0.04659072,  0.00102655],\n",
       "       [-0.10106982, -0.08777186, -0.01066153, ..., -0.02205966,\n",
       "        -0.0074909 , -0.11966011]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>As it was explained, <b>outputs</b> variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The <b>softmax_w</b> shape is [rnn_size, vocab_size], which is [128x65] in our case. Therefore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the <b>softmax(output * softmax_w + softmax_b)</b> for this purpose. The shape of the matrixis would be:</p>\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(6400, 128) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01739901, 0.01804345, 0.01566487, ..., 0.01452478, 0.01637167,\n",
       "        0.01338022],\n",
       "       [0.01498403, 0.01949407, 0.01927022, ..., 0.01444106, 0.01746876,\n",
       "        0.0126975 ],\n",
       "       [0.01872392, 0.02207137, 0.01561657, ..., 0.01506177, 0.02074212,\n",
       "        0.00961418],\n",
       "       ...,\n",
       "       [0.01647118, 0.01623911, 0.01690122, ..., 0.01767405, 0.01986372,\n",
       "        0.01394998],\n",
       "       [0.01359912, 0.01686152, 0.01938508, ..., 0.01260796, 0.01573568,\n",
       "        0.01174388],\n",
       "       [0.01572557, 0.0246017 , 0.01696387, ..., 0.01627016, 0.01475267,\n",
       "        0.01683885]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we are in the position to calculate the cost of training with <b>loss function</b>, and keep feeding the network to learn it. But, the question is: what does the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Okay, by now, you should understand enough about each component of a LSTM network to be able to train it, and predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>All together</h2>\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False, device='/cpu:0'):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 128 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        with tf.device(device):\n",
    "            # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "            basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "            # model.cell.state_size is (128, 128)\n",
    "            self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "            self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "            self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "            # Initial state of the LSTM memory.\n",
    "            # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "            self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "            with tf.variable_scope('rnnlm_class1'):\n",
    "                softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "                softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs] \n",
    "\n",
    "            # The value of state is updated after processing each batch of chars.\n",
    "            outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "            output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "            self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                    [tf.reshape(self.targets, [-1])],\n",
    "                    [tf.ones([batch_size * seq_length])],\n",
    "                    vocab_size)\n",
    "            self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "            self.final_state = last_state\n",
    "            self.lr = tf.Variable(0.0, trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"cpu_vs_gpu\"></a>\n",
    "<h2>Train your model using CPU and GPU</h2>\n",
    "We can train our model through feeding batches. You should be able to complete the following cells and submit it for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_1\"></a>\n",
    "<h2>Question 1: Complete the code to run it on CPU</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.056, time/batch = 0.110\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The rlost ares ant glonees the woik ave the chanoss for hille,\n",
      "Hollie whe simur. Priaw: the laay,\n",
      "\n",
      "ove sonde noll vead,\n",
      "Why novebinn aubred.\n",
      "\n",
      "FOH OLEOOD\n",
      "E\n",
      "----------------------------------\n",
      "347/8700 (epoch 1), train_loss = 1.849, time/batch = 0.080\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The theard endeem's migh, hear hersomy weest oo to me! loun, the tale ppeath a too's framises,\n",
      "He dame, for plarish whesr'd Beers and sone.\n",
      "The simtllatar\n",
      "----------------------------------\n",
      "521/8700 (epoch 2), train_loss = 1.743, time/batch = 0.115\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The vive tirants\n",
      "And notelio stay it the guke of thou on hour metys art headtry homed?\n",
      "\n",
      "DOTHAONT:\n",
      "'twere a uson of I all exetcome,\n",
      "Take one wher fair sif.\n",
      "----------------------------------\n",
      "695/8700 (epoch 3), train_loss = 1.679, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The siffer?\n",
      "\n",
      "MENENIUS:\n",
      "Geaclo, a king\n",
      "sevel it of a tointable framenter bettern, a mours'd yeare's trep'd how. What for with slisss, undonert\n",
      "Tho are a pi\n",
      "----------------------------------\n",
      "869/8700 (epoch 4), train_loss = 1.638, time/batch = 0.113\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wans ondeedering still newled on quist. Are is own his trumper-boare ammousess made true lands, adlebanched and theree we?\n",
      "\n",
      "GREMES:\n",
      "How is dees them. \n",
      "----------------------------------\n",
      "1043/8700 (epoch 5), train_loss = 1.608, time/batch = 0.083\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The when tell the fatwer'd durscurny; haur too lives adly\n",
      "do she of thy father mak is.\n",
      "\n",
      "BDONCETLER:\n",
      "Whom out thus to my truman, latt so not way which to a\n",
      "----------------------------------\n",
      "1217/8700 (epoch 6), train_loss = 1.584, time/batch = 0.119\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wish of my blawion not in thee?\n",
      "\n",
      "PRONCANLER:\n",
      "Nove King on me what, I first; Beentent the fepfece?\n",
      "In an! Engled, holinguge Edwards, which the gone tak\n",
      "----------------------------------\n",
      "1391/8700 (epoch 7), train_loss = 1.564, time/batch = 0.079\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The have my anger abrear,\n",
      "I shill there for has way gall the lock of his;\n",
      "Soft you.\n",
      "\n",
      "ROREO:\n",
      "Ah, shame to your maste?\n",
      "\n",
      "SICINIUS:\n",
      "Ay,\n",
      "Fit, seanges.\n",
      "\n",
      "CLIDAND\n",
      "----------------------------------\n",
      "1565/8700 (epoch 8), train_loss = 1.547, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wank his eity you:\n",
      "Ed, whou hast your havise all\n",
      "his make is so a done.\n",
      "\n",
      "JULIET:\n",
      "A drige of all heard,\n",
      "Day?\n",
      "Ammondew us\n",
      "which you how the precores Cer\n",
      "----------------------------------\n",
      "1739/8700 (epoch 9), train_loss = 1.532, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The will seen of thou, make thee this woe.\n",
      "Lucenter in grace of this lent me's your lo;\n",
      "Though it is friens, Show none to he did himself-doving ursale swa\n",
      "----------------------------------\n",
      "1913/8700 (epoch 10), train_loss = 1.519, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prone..\n",
      "\n",
      "VILAND:\n",
      "Nay chomatain thrue his call, That yet, sir, the could\n",
      "once doous; some nom play respected armist; but feels\n",
      "Lold the lad of thy plea\n",
      "----------------------------------\n",
      "2087/8700 (epoch 11), train_loss = 1.508, time/batch = 0.112\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pright turny, gras.\n",
      "\n",
      "Second ServingE.\n",
      "Speak us that off foul enon throre or\n",
      "'tis lew ut;\n",
      "And you shoupt rest Bannias, the desemble\n",
      "Of makes,\n",
      "And it. C\n",
      "----------------------------------\n",
      "2261/8700 (epoch 12), train_loss = 1.498, time/batch = 0.080\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The but love togatenate,\n",
      "Which you were, even that thou, best all Cousint?\n",
      "By the poor, honter, to me:\n",
      "The king croves,\n",
      "And, if I should you what I plunt \n",
      "----------------------------------\n",
      "2435/8700 (epoch 13), train_loss = 1.490, time/batch = 0.084\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The think town? moath you woundscome by the necled\n",
      "So give never and same not it one ocked to genery\n",
      "Bame, the still not you.\n",
      "\n",
      "CORIOLANUS:\n",
      "Majesty traitor\n",
      "----------------------------------\n",
      "2609/8700 (epoch 14), train_loss = 1.482, time/batch = 0.118\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The injurious Bount.\n",
      "\n",
      "DUKE OF ESPkO:\n",
      "To brovisons yield up, if so thou raugh, swore, whose suck speak prettingly purpised upon For I ground leave more the\n",
      "----------------------------------\n",
      "2783/8700 (epoch 15), train_loss = 1.475, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The will pound thy nathed; that I was know has your eye,\n",
      "I may you that he whole aster's eaters with o'er, nock a hate,\n",
      "So will not withs brattleme in the\n",
      "----------------------------------\n",
      "2957/8700 (epoch 16), train_loss = 1.469, time/batch = 0.082\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pight are sounds.\n",
      "Thy master:\n",
      "Good doth to pity in the senfert to have thy bise accuse off?\n",
      "\n",
      "BRUTUS:\n",
      "I pray,\n",
      "To roves Nors! what is the dugst ever fan\n",
      "----------------------------------\n",
      "3131/8700 (epoch 17), train_loss = 1.464, time/batch = 0.118\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The will sait, three weasing, infe's sale heavy you.\n",
      "\n",
      "ISABELLA:\n",
      "Let through my brother, from King Henry you brish's womd,\n",
      "the worlds\n",
      "What dost talp, and y\n",
      "----------------------------------\n",
      "3305/8700 (epoch 18), train_loss = 1.459, time/batch = 0.107\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Ourry,\n",
      "No rock,\n",
      "And by the oractent invioled morros, but me him.\n",
      "Can besoll my morn young you, is till be the tent gard him to be sonce, sode on?\n",
      "\n",
      "GRE\n",
      "----------------------------------\n",
      "3479/8700 (epoch 19), train_loss = 1.455, time/batch = 0.082\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The joy To be are, and shall stainst thee-hing,--\n",
      "\n",
      "Nurse:\n",
      "Pard shall doxt, inle, do heet.\n",
      "\n",
      "LADY CAPULET:\n",
      "Justy poor brobed me, to sheet that of sweet what\n",
      "----------------------------------\n",
      "3653/8700 (epoch 20), train_loss = 1.451, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The heart Lective and arming\n",
      "that you sime themper use blood!\n",
      "\n",
      "ROMEO:\n",
      "Master of now argues of her things is me, as own to knee:\n",
      "As the cerrager, 'twere\n",
      "As\n",
      "----------------------------------\n",
      "3827/8700 (epoch 21), train_loss = 1.448, time/batch = 0.112\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The one, none shaglo.\n",
      "\n",
      "First Senatold, for Wasting before 'truity that time unknow, what you would ever in his blow the born our to'er young that said: si\n",
      "----------------------------------\n",
      "4001/8700 (epoch 22), train_loss = 1.445, time/batch = 0.115\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Northy pursured let speak,\n",
      "Which pasdip's fly,\n",
      "Thou died\n",
      "with you.\n",
      "\n",
      "PERDITA:\n",
      "I prompass.\n",
      "\n",
      "CORIOLANUS:\n",
      "A blote his sway? Nay to me not, but thou art so\n",
      "----------------------------------\n",
      "4175/8700 (epoch 23), train_loss = 1.442, time/batch = 0.117\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The worse: not was morn agize;\n",
      "Stay'd gid possing of it for sed.\n",
      "My live, now with him.\n",
      "\n",
      "AUFIDIUS:\n",
      "Thou was\n",
      "And will.\n",
      "\n",
      "CORIOLANUS:\n",
      "O uncle, here,\n",
      "Come, th\n",
      "----------------------------------\n",
      "4349/8700 (epoch 24), train_loss = 1.439, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The thost do I children she, be to\n",
      "so seen unty this news, and serster be bagnant of thee,\n",
      "Better assulf brings and come.\n",
      "\n",
      "ESCALUS:\n",
      "Then lusto;\n",
      "Barning ot\n",
      "----------------------------------\n",
      "4523/8700 (epoch 25), train_loss = 1.436, time/batch = 0.081\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Noffins to tugh.\n",
      "\n",
      "First Mumboun, that sky laught,\n",
      "Take is inlehaucior know matter house my deare\n",
      "that I did raze:\n",
      "The droportainly, as thy carrios, an\n",
      "----------------------------------\n",
      "4697/8700 (epoch 26), train_loss = 1.433, time/batch = 0.121\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Richard slow you shall grow's shadly men hence!\n",
      "Now, baggely death,\n",
      "Is or a merpherd and to no long herself.\n",
      "There honest his yelish blood, and being \n",
      "----------------------------------\n",
      "4871/8700 (epoch 27), train_loss = 1.430, time/batch = 0.081\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Camnoking bred: a fatentid nex such;\n",
      "Is thy trumph king;\n",
      "Deven his pouns against the hoor is?\n",
      "How heaves?\n",
      "\n",
      "JULIET:\n",
      "And eforce, more their damis?\n",
      "\n",
      "DORB\n",
      "----------------------------------\n",
      "5045/8700 (epoch 28), train_loss = 1.428, time/batch = 0.118\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The will ten some with slaughter'd, and Retrunk me.\n",
      "Why.'\n",
      "Yee he\n",
      "they steem to blusode the tuntterioning Roman:\n",
      "The trucicies of a merrance now that stall\n",
      "----------------------------------\n",
      "5219/8700 (epoch 29), train_loss = 1.426, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The come home:\n",
      "We are a marshable vaitle let some headed and my daughter\n",
      "An his father; him so had pabveddy\n",
      "Rome which the anto\n",
      "That what you killseld she\n",
      "----------------------------------\n",
      "5393/8700 (epoch 30), train_loss = 1.424, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The give the bound.\n",
      "\n",
      "ISABELLA:\n",
      "I'll be unclewness,\n",
      "Has thy fly,\n",
      "It sweet to Marcius!\n",
      "Think you mile.\n",
      "Should man;\n",
      "Lount'st I cithul: one is? an Wiltoby-hea\n",
      "----------------------------------\n",
      "5567/8700 (epoch 31), train_loss = 1.422, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The have bristed worn ord prince.\n",
      "\n",
      "SICINIUS:\n",
      "Ine conself\n",
      "And prays, let she, come him mine; or Clarence, my green worst' Pothen clove to me, But other sou\n",
      "----------------------------------\n",
      "5741/8700 (epoch 32), train_loss = 1.420, time/batch = 0.111\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The rul\n",
      "Descure\n",
      "It brother, appery cheak by confill's rudent of Necring and discontall'd me;\n",
      "And, silverent low.\n",
      "\n",
      "LUCIO:\n",
      "O, that she very mourner before m\n",
      "----------------------------------\n",
      "5915/8700 (epoch 33), train_loss = 1.418, time/batch = 0.115\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Madward Buckingry game to beny, impose,\n",
      "alivul will courns go that are,\n",
      "One wals the mother,\n",
      "And cause force:\n",
      "My lord.\n",
      "Come, my lord, you your noble d\n",
      "----------------------------------\n",
      "6089/8700 (epoch 34), train_loss = 1.417, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The like pity your\n",
      "come as this long dined for humind you move\n",
      "Agates, you are I of France;\n",
      "Out it.\n",
      "\n",
      "MENENIUS:\n",
      "He shall gree? honest parried in fool.\n",
      "Han \n",
      "----------------------------------\n",
      "6263/8700 (epoch 35), train_loss = 1.415, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Wonder!\n",
      "How but by the may\n",
      "I pleit to him;\n",
      "My baze again!'''ll go contenter,\n",
      "Will, I wind\n",
      "On cousin.\n",
      "I will us: straighted I within!?\n",
      "\n",
      "PROSPERO:\n",
      "Basim\n",
      "----------------------------------\n",
      "6437/8700 (epoch 36), train_loss = 1.414, time/batch = 0.113\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The follow thy father father.\n",
      "\n",
      "Lary his cusfed your streaks\n",
      "advengement:\n",
      "Ay:\n",
      "Tyirn, in this souls for what I\n",
      "that's come will proves?\n",
      "\n",
      "PETRUCHIO:\n",
      "I would \n",
      "----------------------------------\n",
      "6611/8700 (epoch 37), train_loss = 1.413, time/batch = 0.112\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The offence Lancheagn all, good sunsume was would it then, poit and love,\n",
      "And why wouldy out to this same, this was enclament all not make me doth our, I'\n",
      "----------------------------------\n",
      "6785/8700 (epoch 38), train_loss = 1.411, time/batch = 0.113\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The and there\n",
      "To do be part up my lord.\n",
      "\n",
      "GREJOH:\n",
      "Fortune\n",
      "Mother.\n",
      "\n",
      "LADY ANNE:\n",
      "Sweet so foops:\n",
      "For I but to, by the success would hear upon Xerit a post gri\n",
      "----------------------------------\n",
      "6959/8700 (epoch 39), train_loss = 1.410, time/batch = 0.118\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Lord; ask mirth you, sir! whither let's death mine eye heaven do sk hand\n",
      "of well thou wilt take her\n",
      "that by brother, which I'll puckern trremetlimatin\n",
      "----------------------------------\n",
      "7133/8700 (epoch 40), train_loss = 1.409, time/batch = 0.118\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The off'll not will you sits that with a buthness. Penitus put ovil, and all his learna addles lighting a got shall not is sorrow the barron slain?\n",
      "Well, \n",
      "----------------------------------\n",
      "7307/8700 (epoch 41), train_loss = 1.408, time/batch = 0.080\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Lady but friend of nothing\n",
      "For drea\n",
      "Disposside in true complitain givent. Where you clop scalls away, at twill Richard;\n",
      "To comars! son am a swearly.\n",
      "\n",
      "\n",
      "----------------------------------\n",
      "7481/8700 (epoch 42), train_loss = 1.407, time/batch = 0.117\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Northumberland!\n",
      "\n",
      "QUEEN ELIZABETH,\n",
      "That you are him to pay say,\n",
      "Further, have felive at with his crown?\n",
      "\n",
      "Messenger: then all oats the least\n",
      "Well thee f\n",
      "----------------------------------\n",
      "7655/8700 (epoch 43), train_loss = 1.406, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The villain of thy butters,\n",
      "Beauty!\n",
      "\n",
      "FLORIZEL:\n",
      "For't:\n",
      "Whurt to grahe else against a boyal little nungs indreased infately deaths's name: therefore crevied\n",
      "----------------------------------\n",
      "7829/8700 (epoch 44), train_loss = 1.405, time/batch = 0.115\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Crdrough:\n",
      "By one more enement made in;\n",
      "For away't my land tells! now sort to swold thy footh, to come on my revenge; equire imprised of the carent, yo\n",
      "----------------------------------\n",
      "8003/8700 (epoch 45), train_loss = 1.404, time/batch = 0.116\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The watch you! from the ormenis, e'er true to changers remember foolist discovera'd that say Frowns of Him:\n",
      "And your lie praiderous dackle than true upon \n",
      "----------------------------------\n",
      "8177/8700 (epoch 46), train_loss = 1.404, time/batch = 0.114\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The thorse my lord, a wender, if I heep--\n",
      "This sons that stuckner,\n",
      "Sun to Doo doubt\n",
      "Aunder with the lord,\n",
      "You hold me in meun.\n",
      "\n",
      "KATHARINA:\n",
      "My times,\n",
      "From \n",
      "----------------------------------\n",
      "8351/8700 (epoch 47), train_loss = 1.403, time/batch = 0.117\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The anger that is English my mourner ore of a clease\n",
      "the should be hurse in heart my tale, ere richel.\n",
      "\n",
      "IAG Bousides, confess threw as?\n",
      "\n",
      "KONANG:\n",
      "Knoo, and\n",
      "----------------------------------\n",
      "8525/8700 (epoch 48), train_loss = 1.402, time/batch = 0.111\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Dush these mistress, consulat\n",
      "He kind of tweeding one lords, these.\n",
      "He, pees pray us\n",
      "accoval and Mistress'd place reasors. Pray.\n",
      "What, like akly hank \n",
      "----------------------------------\n",
      "8699/8700 (epoch 49), train_loss = 1.401, time/batch = 0.083\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The lide importune compariciah;\n",
      "Are you, stranging.\n",
      "For have very's date us, on the Vession us\n",
      "Thou could we threager do the Rome; noons?\n",
      "See ne'er loyall\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "avg_batch_running_duration_CPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn_CPU\"):\n",
    "    model = LSTMModel(device='/cpu:0')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        ## write your code bellow to reset the batch pointer in data_loader. you can use reset_batch_pointer()\n",
    "        ##\n",
    "        ##\n",
    "        \n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        state = sess.run(model.initial_state) # model initialization\n",
    "        batch_running_duration_CPU = []\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            ## write your code to define your x and y. You should use next_batch() from data_loader\n",
    "            ## e.g. x,y =\n",
    "            ##\n",
    "            \n",
    "            x,y = data_loader.next_batch()\n",
    "            \n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            \n",
    "            ## write your code to train the model\n",
    "            ## fe.g.: train_loss, state, _ = \n",
    "            ##\n",
    "\n",
    "            train_loss, state, _ = sess.run(\n",
    "                [model.cost, model.final_state, model.train_op],\n",
    "                feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            \n",
    "            ## write your code to store the duration of runing each batch in a list (end - start)\n",
    "            ##\n",
    "            ##\n",
    "            \n",
    "            batch_running_duration_CPU.append(end - start)\n",
    "            \n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        avg_batch_running_duration_CPU.append(sum(batch_running_duration_CPU) / float(len(batch_running_duration_CPU)))\n",
    "        \n",
    "        # Please uncomment the following block of the code so the grader can see the sample of prediction\n",
    "        with tf.variable_scope(\"rnn_CPU\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print ('----------------------------------')\n",
    "            print ('SAMPLE GENERATED TEXT:')\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_2\"></a>\n",
    "<h2>Question 2: Complete the code to run it on GPU</h2>\n",
    "Now, create the same network with GPU, and calculate the time/batch for running each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.023, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The bun the efand\n",
      "Whingald,\n",
      "As I nosteabbest\n",
      "To wrot teitle'd trecp hewile, anl aptify formeven om woth thou grof my crowor herhfon, wick'd the is the re.\n",
      "----------------------------------\n",
      "347/8700 (epoch 1), train_loss = 1.824, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Budys tham now\n",
      "Thme erraut? am fermand gox this ceans then we your like,\n",
      "And then kiry oor har thoust rame of I cus-in the will some for hay knot be t\n",
      "----------------------------------\n",
      "521/8700 (epoch 2), train_loss = 1.725, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The know'd bewpe,\n",
      "Will fainter him:\n",
      "\n",
      "Leopll ard beath, thet lood the will balghabrech this mistrey.\n",
      "\n",
      "HeRO III:\n",
      "Nay'd day from Corawint ol feak in mus it.\n",
      "\n",
      "----------------------------------\n",
      "695/8700 (epoch 3), train_loss = 1.667, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The with uponouth, now up didet fore trumined call of his may\n",
      "To dotbyer, outo an heart and this larger grarminged you chiece to roon clints thank?\n",
      "\n",
      "First\n",
      "----------------------------------\n",
      "869/8700 (epoch 4), train_loss = 1.628, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dething amplike of York, Clawenty anso, if the compely, you\n",
      "leaves this bawch, it quirn't,\n",
      "I propen thou Ferenty. Give to sourthing and love,\n",
      "He gepol\n",
      "----------------------------------\n",
      "1043/8700 (epoch 5), train_loss = 1.596, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The fearlow, my lord;\n",
      "Thy blord:\n",
      "My look, call that. Go would known and grawn,\n",
      "Master the use to usI well Iply with the\n",
      "dead,\n",
      "And some vaneo-ther, first t\n",
      "----------------------------------\n",
      "1217/8700 (epoch 6), train_loss = 1.570, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The you gragum: the such thou dafficed us.\n",
      "\n",
      "KING RICHARD III:\n",
      "On call be lard, I man with your croeme; the suivinature arfais;\n",
      "My lord, tentle come can yo\n",
      "----------------------------------\n",
      "1391/8700 (epoch 7), train_loss = 1.550, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The say, thill but quince acted one nonoul:\n",
      "O, kindle, the pursnady. That me look'st the sun: you are say on the word; here?\n",
      "\n",
      "First Corungs thy inforce sa\n",
      "----------------------------------\n",
      "1565/8700 (epoch 8), train_loss = 1.533, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The are that that am in volmon, a pray,--\n",
      "A lafminted by opprish thou fear-consort.\n",
      "\n",
      "BIONDELLO:\n",
      "Who sir I shall not be gone alles,\n",
      "Youb tirl I to this bro\n",
      "----------------------------------\n",
      "1739/8700 (epoch 9), train_loss = 1.518, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The him the bollong'sed of grain thee\n",
      "well with 'tis standil remains for the king exblen?\n",
      "\n",
      "Secomen:\n",
      "Wearfain gave the brucking,\n",
      "And will king any a verrac\n",
      "----------------------------------\n",
      "1913/8700 (epoch 10), train_loss = 1.506, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The eacter land, which sime arm'd, and thence, my lord.\n",
      "\n",
      "PeOPhe: what sbitch, alford with founds you so resol!,\n",
      "That beaus, looks of into thet quietle, we\n",
      "----------------------------------\n",
      "2087/8700 (epoch 11), train_loss = 1.495, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The stound you true by the Richard is hols?\n",
      "\n",
      "DUKE OF YOSS:\n",
      "I can come arm'd.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "'Twilk:\n",
      "Ha, not I is mark upon went the consturion of the ju\n",
      "----------------------------------\n",
      "2261/8700 (epoch 12), train_loss = 1.486, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The your king.\n",
      "When the king's heedance! do you have. Where coman\n",
      "Menis their look: a banger than for\n",
      "Rome, I have keeple;\n",
      "The name amon me, sir,\n",
      "You To m\n",
      "----------------------------------\n",
      "2435/8700 (epoch 13), train_loss = 1.478, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The queinio Cakins him his\n",
      "placely a son:\n",
      "Now is nor not, the with hate Fast find was unandsy sun thy fair?\n",
      "Save O, to Glough,\n",
      "Who shawe misciuss Sicus th\n",
      "----------------------------------\n",
      "2609/8700 (epoch 14), train_loss = 1.472, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The womb,\n",
      "And, an all.\n",
      "'Lose thy brok\n",
      "On Gone unlaw to die, with the peak the wanting of my Maison,\n",
      "With children;\n",
      "But he, take a was by the devence.\n",
      "\n",
      "She\n",
      "----------------------------------\n",
      "2783/8700 (epoch 15), train_loss = 1.466, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Marcay to presenger of the many.\n",
      "'Tis now lave with us or merchart's\n",
      "Yet thy hanges on now after to his stood whose huster than the mmishmen both?\n",
      "Sy \n",
      "----------------------------------\n",
      "2957/8700 (epoch 16), train_loss = 1.460, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The isure that the bastard, theirs.\n",
      "\n",
      "MARCIUS:\n",
      "Good put one shape;\n",
      "Well be proud the tignion.\n",
      "I am distand was-day:\n",
      "Friar, those tong.\n",
      "Cheldere king so thi\n",
      "----------------------------------\n",
      "3131/8700 (epoch 17), train_loss = 1.455, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The say suspect Master about we sum: nose my corse of this wrove: thou with a noble his use of the instrughts\n",
      "that almour tongue's Poir protecot with thec\n",
      "----------------------------------\n",
      "3305/8700 (epoch 18), train_loss = 1.450, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The with colaces I thought accolds with unpain flage.\n",
      "\n",
      "GLOETk:\n",
      "Why how know the fearle,\n",
      "And that strike, to Borres I prothmed worself.\n",
      "\n",
      "CATESBY:\n",
      "Yea is th\n",
      "----------------------------------\n",
      "3479/8700 (epoch 19), train_loss = 1.446, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The eas thousing reasons, but, most to Burpassicy to\n",
      "come? I have base:\n",
      "Have being unon the fine,\n",
      "'Tis serve for a let hurt to fals his falstury, what, is\n",
      "----------------------------------\n",
      "3653/8700 (epoch 20), train_loss = 1.442, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The no, no mort, you'r man his breath the strong\n",
      "The prince, and his body of rether, great Warwick-sherty\n",
      "An often\n",
      "pather leaves outh\n",
      "Your father, for sou\n",
      "----------------------------------\n",
      "3827/8700 (epoch 21), train_loss = 1.439, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The made and by tiul, by the weven the rucking almorred, and then thou, Katy!\n",
      "\n",
      "LEONTES:\n",
      "How farth, for York a mother;\n",
      "And, no Hentter\n",
      "To be vely often? Th\n",
      "----------------------------------\n",
      "4001/8700 (epoch 22), train_loss = 1.436, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The him.\n",
      "\n",
      "CORIOLANUS:\n",
      "Hence the no name\n",
      "With deself\n",
      "Take you, in whose is our ower than you\n",
      "But battled of your\n",
      "Your death;\n",
      "Once finger from the wish,\n",
      "We \n",
      "----------------------------------\n",
      "4175/8700 (epoch 23), train_loss = 1.433, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The med so mink-play as fey, faftiniage my husbent of worax the glory\n",
      "For I poorted-alous againly gol!\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "At away,\n",
      "My Lord looks,\n",
      "Ont hat\n",
      "----------------------------------\n",
      "4349/8700 (epoch 24), train_loss = 1.430, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The shalt's new like you have bear his father, to the over's scards we are: then, but I am soldier Harry I may who idlels; there, with are your serving co\n",
      "----------------------------------\n",
      "4523/8700 (epoch 25), train_loss = 1.428, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sing.\n",
      "\n",
      "LADY ANNE:\n",
      "Well takness?\n",
      "\n",
      "SICINARDO:\n",
      "Nay. Bady, from the queen with thy poison the garlands,\n",
      "That's such of his bons the father,\n",
      "And, let me te\n",
      "----------------------------------\n",
      "4697/8700 (epoch 26), train_loss = 1.425, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The shall prisant spul; lew you are.\n",
      "\n",
      "MARCIUS:\n",
      "Faith the presence: Ox you reverenced and full to-morrow, and to come musiand\n",
      "Which seprece to men. 'Tis no\n",
      "----------------------------------\n",
      "4871/8700 (epoch 27), train_loss = 1.423, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The make Here is my words, take disgbent\n",
      "Not vengeance: so.\n",
      "\n",
      "ClAPELIF:\n",
      "Whater the majesty in great Margard blood father.\n",
      "\n",
      "SICINIUS:\n",
      "Go of Clarvoy:\n",
      "What su\n",
      "----------------------------------\n",
      "5045/8700 (epoch 28), train_loss = 1.421, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The have I minn with what speak?\n",
      "\n",
      "LUCIO:\n",
      "I shall be wlat as out,\n",
      "O, sound harves, go to splant thy givbour bride.\n",
      "Make our hearts at the vipition my\n",
      "fares\n",
      "----------------------------------\n",
      "5219/8700 (epoch 29), train_loss = 1.419, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dully is truther words shall spent chevies waten.\n",
      "Couldering not when miesiod your mother most to more trumpes to much thy fair grieve thus plantage,\n",
      "\n",
      "----------------------------------\n",
      "5393/8700 (epoch 30), train_loss = 1.417, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prepence years!\n",
      "So stand on where their uncle crows to am but Pinish'd\n",
      "And like so fear upon melfie\n",
      "Do dyor could not seatly sentence.\n",
      "\n",
      "DUCHESS OF YOR\n",
      "----------------------------------\n",
      "5567/8700 (epoch 31), train_loss = 1.416, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The seemech in\n",
      "sper-upon this hangers, let's barge: it do fair pinan\n",
      "From: have tell thee\n",
      "Which instrupher's mother Haster, sir, by thy weard Montague to \n",
      "----------------------------------\n",
      "5741/8700 (epoch 32), train_loss = 1.414, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The bear with the hurse,\n",
      "Should know meo-noo\n",
      "Farrights.\n",
      "\n",
      "Nurse:\n",
      "Ay, it wast should begins he thousel, I did do\n",
      "Are mysurne!\n",
      "\n",
      "CLARENCE:\n",
      "I?\n",
      "Proceary?\n",
      "\n",
      "CORIO\n",
      "----------------------------------\n",
      "5915/8700 (epoch 33), train_loss = 1.412, time/batch = 0.020\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The quickly be me to rock proved;\n",
      "Of noble command? and we of mock.\n",
      "\n",
      "WETENES:\n",
      "Designadt, it is such before\n",
      "have pronine atterothen, legward'st the hand we\n",
      "----------------------------------\n",
      "6089/8700 (epoch 34), train_loss = 1.411, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The defents, ready Capulely thy one in a businesses, siccuman! withaliber give,\n",
      "Of much any tabour, and Edward to thy sorrow,--\n",
      "PHilk him to such trust,\n",
      "W\n",
      "----------------------------------\n",
      "6263/8700 (epoch 35), train_loss = 1.410, time/batch = 0.022\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The hath tasted\n",
      "How Londom Duke to see for hither Wargale.\n",
      "See to cost of Richial house,\n",
      "The inimpd and she's fair attether,\n",
      "Stein away! If I come to do\n",
      "A\n",
      "----------------------------------\n",
      "6437/8700 (epoch 36), train_loss = 1.408, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dead blidam Lucenty Henry's\n",
      "Of abouts in undone.\n",
      "\n",
      "CLARENCE:\n",
      "A truth, fire what upon the waw you hire;\n",
      "And I am here and records; there is not? and tel\n",
      "----------------------------------\n",
      "6611/8700 (epoch 37), train_loss = 1.407, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The for the point.\n",
      "\n",
      "GREGORY:\n",
      "Gold deserved, what, loid of clouge, contrathan their gide:\n",
      "What is it, by the eases! O shor athrre not?\n",
      "\n",
      "LADY CAPULET:\n",
      "Go hi\n",
      "----------------------------------\n",
      "6785/8700 (epoch 38), train_loss = 1.405, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The is with highty is the life.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Here yet whee, hop you within.\n",
      "Welland. The with me?\n",
      "\n",
      "KING HOPHOR bitterring sister, as the pear of Roman\n",
      "----------------------------------\n",
      "6959/8700 (epoch 39), train_loss = 1.404, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The your tith, and dirn,\n",
      "And mephora?--O shall have blow you\n",
      "Of instruced full leats'd us that longain pedness\n",
      "For he can hie razk through, he did affecti\n",
      "----------------------------------\n",
      "7133/8700 (epoch 40), train_loss = 1.403, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The may; make barked that desire Lame them we ardedly elder, beats word\n",
      "Connow at the lossive,\n",
      "And enemy, I'll I'll perceive against to sheal biteth tower\n",
      "----------------------------------\n",
      "7307/8700 (epoch 41), train_loss = 1.402, time/batch = 0.021\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The were no very besides\n",
      "Thy didst thou throw'st thou where hulls on these judy,\n",
      "The horrion quiet? must look'st upon outh! O my leavenous virging out, if\n",
      "----------------------------------\n",
      "7481/8700 (epoch 42), train_loss = 1.401, time/batch = 0.018\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Vortunisars,\n",
      "To more is by please,\n",
      "Did said no tlant, the brother's troyorate, and now are an will not stay by my angers?\n",
      "\n",
      "HASTINIANLEBO:\n",
      "Even sting n\n",
      "----------------------------------\n",
      "7655/8700 (epoch 43), train_loss = 1.400, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Hessour him.\n",
      "\n",
      "KING HENRY VI:\n",
      "She! where shall partice;\n",
      "At thou depord;\n",
      "Conform.\n",
      "\n",
      "SICINIUS: Vay the revilio.\n",
      "\n",
      "CAPULET:\n",
      "Farelia, the hould that I know h\n",
      "----------------------------------\n",
      "7829/8700 (epoch 44), train_loss = 1.399, time/batch = 0.023\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The to side, for I should by thy day;\n",
      "For you with post,\n",
      "Plant is garling stroke at hour unto bring it because, my bloody.\n",
      "But to twent you.\n",
      "\n",
      "MENENIUS:\n",
      "Du\n",
      "----------------------------------\n",
      "8003/8700 (epoch 45), train_loss = 1.399, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The a quagured\n",
      "And trant.\n",
      "\n",
      "LADY GREY:\n",
      "Juliet\n",
      "Should not for the countest,\n",
      "Ladw we have of\n",
      "take him because,\n",
      "His she:\n",
      "I had it blood\n",
      "With love: whereof lon\n",
      "----------------------------------\n",
      "8177/8700 (epoch 46), train_loss = 1.398, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The distainlike and sacriper,\n",
      "Of this tale,\n",
      "And I worselvest Cale? you'll seize her known, which yet thou shalt gues and my heart your obse Aumbend; alcon\n",
      "----------------------------------\n",
      "8351/8700 (epoch 47), train_loss = 1.397, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The vies, and 'towarm's fiet, not arm'd comes with my baws and sweath! I come,\n",
      "Not demiled\n",
      "Who aspe-not or this, your little chose's blood Isare to beseec\n",
      "----------------------------------\n",
      "8525/8700 (epoch 48), train_loss = 1.396, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The beder; then they be.\n",
      "Sir, and shall do you have done, I am nothing of complaint.\n",
      "I year in wind at ome are commines, how Ascelossess lived\n",
      "And buriy i\n",
      "----------------------------------\n",
      "8699/8700 (epoch 49), train_loss = 1.396, time/batch = 0.019\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The sistor:\n",
      "So will lies her hands; more throps of the presence was ask.\n",
      "Beforiod bend go countryman, by the severety of Claison away.\n",
      "Good cramposed,\n",
      "To \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Create the same network using GPU\n",
    "\n",
    "\n",
    "avg_batch_running_duration_GPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"rnn_GPU\"):\n",
    "    model = LSTMModel(device='/gpu:0')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        ## write your code bellow to reset the batch pointer in data_loader. you can use reset_batch_pointer()\n",
    "        ##\n",
    "        ##\n",
    "\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        state = sess.run(model.initial_state) # model initialization\n",
    "        batch_running_duration_GPU = []\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            ## write your code to define your x and y. You should use next_batch() from data_loader\n",
    "            ## e.g. x,y =\n",
    "            ##\n",
    "\n",
    "            x,y = data_loader.next_batch()\n",
    "            \n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}            \n",
    "            ## write your code to train the model\n",
    "            ## fe.g.: train_loss, state, _ = \n",
    "            ##\n",
    "\n",
    "            train_loss, state, _ = sess.run(\n",
    "                [model.cost, model.final_state, model.train_op],\n",
    "                feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            ## write your code to store the duration of runing each batch in a list (end - start)\n",
    "            ##\n",
    "            ##\n",
    "\n",
    "            batch_running_duration_GPU.append(end - start)\n",
    "            \n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        avg_batch_running_duration_GPU.append(sum(batch_running_duration_GPU) / float(len(batch_running_duration_GPU)))\n",
    "        \n",
    "        # Please uncomment the following block of the code so the grader can see the sample of prediction\n",
    "        with tf.variable_scope(\"rnn_GPU\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print ('----------------------------------')\n",
    "            print ('SAMPLE GENERATED TEXT:')\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_3\"></a>\n",
    "<h2>Question 3: Compare the results</h2>\n",
    "Finally, using a graph, show the speed of training (batch/time) for the model running on GPU and CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3eff19beb320>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your code here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=[7,5])\n",
    "axis_1 = fig.add_axes([0,0,1,1])\n",
    "axis_1.set_xlabel(\"epoch\")\n",
    "axis_1.set_ylabel(\"time\")\n",
    "axis_1.plot(np.array(avg_batch_running_duration_CPU), label=\"Batch/time for CPU\")\n",
    "axis_1.plot(np.array(avg_batch_running_duration_GPU), label=\"Batch/time for GPU\")\n",
    "axis_1.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>Do you want to use GPU in production?</h2>\n",
    "\n",
    "<p>Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The <a href=\"https://cocl.us/ML0122EN_IBMCLOUD_PowerAI\">PowerAI platform on IBM Cloud</a> supports popular machine learning libraries and dependencies including TensorFlow, Caffe, PyTorch, and Theano.</p>\n",
    "\n",
    "<h3>Thanks for completing this lesson!</h3>\n",
    "\n",
    "\n",
    "\n",
    "<h4>Author:  <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>,   <a href=\"https://www.linkedin.com/in/yi-leng-yao-84451275/\">Yi leng Yao</a></h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "<p>Copyright &copy; 2018 <a href=\"https://cocl.us/DX0108EN_CC\">Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
